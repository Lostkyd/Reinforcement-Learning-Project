{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lostkyd/Reinforcement-Learning-Project/blob/main/Doom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LD2yXR226xE",
        "outputId": "542177a6-b6f5-48bd-b077-2e2a74d38fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZDkwv-jy1y"
      },
      "source": [
        "# Deep Q learning with Doom üïπÔ∏è\n",
        "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
        "Our agent playing Doom:\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"assets/doom.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxPMGfgKkaq"
      },
      "source": [
        "## For Google Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjsRWCa9n9UM",
        "outputId": "a2653aec-021f-4626-cc77-a9d1c6727a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libbz2-dev is already the newest version (1.0.8-2).\n",
            "libbz2-dev set to manually installed.\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "libjpeg-dev set to manually installed.\n",
            "build-essential is already the newest version (12.8ubuntu1.1).\n",
            "cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n",
            "git is already the newest version (1:2.25.1-1ubuntu3.11).\n",
            "tar is already the newest version (1.30+dfsg-7ubuntu0.20.04.3).\n",
            "unzip is already the newest version (6.0-25ubuntu1.1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.5).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  autopoint debhelper dh-autoreconf dh-strip-nondeterminism dwz\n",
            "  fluid-soundfont-gm freepats gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-harfbuzz-0.0\n",
            "  gir1.2-ibus-1.0 gir1.2-pango-1.0 intltool-debian libao-common libao4\n",
            "  libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev libblkid-dev\n",
            "  libblkid1 libcairo-script-interpreter2 libcairo2-dev libcroco3 libdatrie-dev\n",
            "  libdbus-1-dev libdebhelper-perl libegl1-mesa-dev libffi-dev\n",
            "  libfile-stripnondeterminism-perl libfluidsynth2 libfribidi-dev\n",
            "  libgail-common libgail18 libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-dev\n",
            "  libgl1-mesa-dev libgles-dev libgles1 libgles2-mesa-dev libglib2.0-dev\n",
            "  libglib2.0-dev-bin libglvnd-dev libgraphite2-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libharfbuzz-dev libharfbuzz-gobject0 libharfbuzz-icu0\n",
            "  libibus-1.0-5 libibus-1.0-dev libinstpatch-1.0-2 liblzo2-2\n",
            "  libmail-sendmail-perl libmount-dev libmount1 libopengl-dev libpango1.0-dev\n",
            "  libpangoxft-1.0-0 libpixman-1-dev libpulse-dev libpulse-mainloop-glib0\n",
            "  libselinux1-dev libsepol1-dev libsndio-dev libsub-override-perl\n",
            "  libsys-hostname-long-perl libthai-dev libtool libudev-dev libudev1\n",
            "  libwayland-bin libwayland-dev libwildmidi-config libwildmidi2\n",
            "  libxcb-render0-dev libxcb-shm0-dev libxcomposite-dev libxcursor-dev\n",
            "  libxdamage-dev libxfixes-dev libxi-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxml2-utils libxrandr-dev libxv-dev libxxf86vm-dev pango1.0-tools\n",
            "  po-debconf timgm6mb-soundfont x11proto-input-dev x11proto-randr-dev\n",
            "  x11proto-xf86vidmode-dev x11proto-xinerama-dev\n",
            "Suggested packages:\n",
            "  dh-make fluid-soundfont-gs fluidsynth gettext-doc libasprintf-dev\n",
            "  libgettextpo-dev libaudio2 libsndio6.1 libcairo2-doc libdatrie-doc\n",
            "  libgirepository1.0-dev libglib2.0-doc libgraphite2-utils gvfs libgtk2.0-doc\n",
            "  imagemagick libpango1.0-doc libthai-doc libtool-doc gcj-jdk libwayland-doc\n",
            "  libmail-box-perl musescore pmidi timidity-daemon\n",
            "The following NEW packages will be installed:\n",
            "  autopoint debhelper dh-autoreconf dh-strip-nondeterminism dwz\n",
            "  fluid-soundfont-gm freepats gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-harfbuzz-0.0\n",
            "  gir1.2-ibus-1.0 gir1.2-pango-1.0 intltool-debian libao-common libao4\n",
            "  libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev libblkid-dev\n",
            "  libcairo-script-interpreter2 libcairo2-dev libcroco3 libdatrie-dev\n",
            "  libdbus-1-dev libdebhelper-perl libegl1-mesa-dev libffi-dev\n",
            "  libfile-stripnondeterminism-perl libfluidsynth-dev libfluidsynth2\n",
            "  libfribidi-dev libgail-common libgail18 libgdk-pixbuf2.0-bin\n",
            "  libgdk-pixbuf2.0-dev libgl1-mesa-dev libgles-dev libgles1 libgles2-mesa-dev\n",
            "  libglib2.0-dev libglib2.0-dev-bin libglvnd-dev libgme-dev libgraphite2-dev\n",
            "  libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libgtk2.0-dev libharfbuzz-dev\n",
            "  libharfbuzz-gobject0 libharfbuzz-icu0 libibus-1.0-5 libibus-1.0-dev\n",
            "  libinstpatch-1.0-2 liblzo2-2 libmail-sendmail-perl libmount-dev\n",
            "  libopenal-dev libopengl-dev libpango1.0-dev libpangoxft-1.0-0\n",
            "  libpixman-1-dev libpulse-dev libpulse-mainloop-glib0 libsdl2-dev\n",
            "  libselinux1-dev libsepol1-dev libsndio-dev libsub-override-perl\n",
            "  libsys-hostname-long-perl libthai-dev libtool libudev-dev libwayland-bin\n",
            "  libwayland-dev libwildmidi-config libwildmidi-dev libwildmidi2\n",
            "  libxcb-render0-dev libxcb-shm0-dev libxcomposite-dev libxcursor-dev\n",
            "  libxdamage-dev libxfixes-dev libxi-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxml2-utils libxrandr-dev libxv-dev libxxf86vm-dev nasm pango1.0-tools\n",
            "  po-debconf timgm6mb-soundfont timidity x11proto-input-dev x11proto-randr-dev\n",
            "  x11proto-xf86vidmode-dev x11proto-xinerama-dev\n",
            "The following packages will be upgraded:\n",
            "  libblkid1 libmount1 libudev1\n",
            "3 upgraded, 104 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 169 MB of archives.\n",
            "After this operation, 266 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libblkid1 amd64 2.34-0.1ubuntu9.4 [137 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libmount1 amd64 2.34-0.1ubuntu9.4 [150 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.22 [75.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 gettext-base amd64 0.19.8.1-10build1 [50.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 autopoint all 0.19.8.1-10build1 [412 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libtool all 2.4.6-14 [161 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 dh-autoreconf all 19 [16.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libdebhelper-perl all 12.10ubuntu1 [62.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libarchive-zip-perl all 1.67-2 [90.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libsub-override-perl all 0.09-2 [9,532 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libfile-stripnondeterminism-perl all 1.7.0-1 [15.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 dh-strip-nondeterminism all 1.7.0-1 [5,228 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 dwz amd64 0.13-5 [151 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 libcroco3 amd64 0.6.13-1 [82.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 gettext amd64 0.19.8.1-10build1 [895 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 intltool-debian all 0.35.0+20060710.5 [24.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 po-debconf all 1.0.21 [233 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 debhelper all 12.10ubuntu1 [877 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal/universe amd64 fluid-soundfont-gm all 3.1-5.1 [119 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal/universe amd64 freepats all 20060219-1 [29.0 MB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 gir1.2-atk-1.0 amd64 2.35.1-1ubuntu2 [18.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gir1.2-freedesktop amd64 1.64.1-1~ubuntu20.04.1 [19.2 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gir1.2-gdkpixbuf-2.0 amd64 2.40.0+dfsg-3ubuntu0.4 [8,272 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 libgtk2.0-common all 2.24.32-4ubuntu4 [126 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu focal/main amd64 libpangoxft-1.0-0 amd64 1.44.7-2ubuntu4 [18.0 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 gir1.2-pango-1.0 amd64 1.44.7-2ubuntu4 [26.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu focal/main amd64 libgtk2.0-0 amd64 2.24.32-4ubuntu4 [1,791 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu focal/universe amd64 gir1.2-gtk-2.0 amd64 2.24.32-4ubuntu4 [172 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gir1.2-harfbuzz-0.0 amd64 2.6.4-1ubuntu4.2 [26.4 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libibus-1.0-5 amd64 1.5.22-2ubuntu2.1 [153 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gir1.2-ibus-1.0 amd64 1.5.22-2ubuntu2.1 [65.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu focal/main amd64 libao-common all 1.2.2+20180113-1ubuntu1 [6,644 B]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu focal/main amd64 libao4 amd64 1.2.2+20180113-1ubuntu1 [35.1 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu focal/main amd64 libarchive-cpio-perl all 0.10-1 [9,644 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu focal/main amd64 libffi-dev amd64 3.3-4 [57.0 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-dev-bin amd64 2.64.6-1~ubuntu20.04.6 [109 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libblkid-dev amd64 2.34-0.1ubuntu9.4 [167 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libmount-dev amd64 2.34-0.1ubuntu9.4 [177 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsepol1-dev amd64 3.0-1ubuntu0.1 [325 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu focal/main amd64 libselinux1-dev amd64 3.0-1build2 [151 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglib2.0-dev amd64 2.64.6-1~ubuntu20.04.6 [1,507 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu focal/main amd64 libatk1.0-dev amd64 2.35.1-1ubuntu2 [95.2 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu focal/main amd64 liblzo2-2 amd64 2.10-2 [50.8 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu focal/main amd64 libcairo-script-interpreter2 amd64 1.16.0-4ubuntu1 [54.2 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpixman-1-dev amd64 0.38.4-0ubuntu2.1 [243 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-render0-dev amd64 1.14-2 [18.4 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcb-shm0-dev amd64 1.14-2 [6,716 B]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu focal/main amd64 libcairo2-dev amd64 1.16.0-4ubuntu1 [627 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu focal/main amd64 libdatrie-dev amd64 0.2.12-3 [17.6 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdbus-1-dev amd64 1.12.16-2ubuntu2.3 [167 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgles1 amd64 1.3.2-1~ubuntu0.20.04.2 [10.3 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgles-dev amd64 1.3.2-1~ubuntu0.20.04.2 [47.9 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libopengl-dev amd64 1.3.2-1~ubuntu0.20.04.2 [3,584 B]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglvnd-dev amd64 1.3.2-1~ubuntu0.20.04.2 [11.6 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libegl1-mesa-dev amd64 21.2.6-0ubuntu0.1~20.04.2 [7,760 B]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu focal/universe amd64 libinstpatch-1.0-2 amd64 1.1.2-2build1 [238 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu focal/universe amd64 timgm6mb-soundfont all 1.3-3 [5,420 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu focal/universe amd64 libfluidsynth2 amd64 2.1.1-2 [198 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu focal/universe amd64 libfluidsynth-dev amd64 2.1.1-2 [26.0 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libfribidi-dev amd64 1.0.8-2ubuntu0.1 [62.3 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu focal/main amd64 libgail18 amd64 2.24.32-4ubuntu4 [14.7 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu focal/main amd64 libgail-common amd64 2.24.32-4ubuntu4 [116 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgdk-pixbuf2.0-bin amd64 2.40.0+dfsg-3ubuntu0.4 [14.1 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgdk-pixbuf2.0-dev amd64 2.40.0+dfsg-3ubuntu0.4 [43.6 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgles2-mesa-dev amd64 21.2.6-0ubuntu0.1~20.04.2 [6,424 B]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgme-dev amd64 0.6.2-1build1 [5,804 B]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu focal/main amd64 libgraphite2-dev amd64 1.3.13-11build1 [14.7 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu focal/main amd64 libgtk2.0-bin amd64 2.24.32-4ubuntu4 [7,728 B]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libharfbuzz-icu0 amd64 2.6.4-1ubuntu4.2 [5,580 B]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libharfbuzz-gobject0 amd64 2.6.4-1ubuntu4.2 [20.4 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libharfbuzz-dev amd64 2.6.4-1ubuntu4.2 [526 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu focal/main amd64 libthai-dev amd64 0.1.28-3 [24.5 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu focal/main amd64 pango1.0-tools amd64 1.44.7-2ubuntu4 [26.2 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu focal/main amd64 libpango1.0-dev amd64 1.44.7-2ubuntu4 [132 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu focal/main amd64 x11proto-xinerama-dev all 2019.2-1ubuntu1 [2,628 B]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu focal/main amd64 libxinerama-dev amd64 2:1.1.4-2 [7,896 B]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfixes-dev amd64 1:5.0.3-2 [11.4 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu focal/main amd64 x11proto-input-dev all 2019.2-1ubuntu1 [2,628 B]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu focal/main amd64 libxi-dev amd64 2:1.7.10-0ubuntu1 [187 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu focal/main amd64 x11proto-randr-dev all 2019.2-1ubuntu1 [2,620 B]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu focal/main amd64 libxrandr-dev amd64 2:1.5.2-0ubuntu1 [25.0 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcursor-dev amd64 1:1.2.0-2 [26.5 kB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu focal/main amd64 libxcomposite-dev amd64 1:0.4.5-1 [9,152 B]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu focal/main amd64 libxdamage-dev amd64 1:1.1.5-2 [5,228 B]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxml2-utils amd64 2.9.10+dfsg-5ubuntu0.20.04.6 [37.0 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu focal/universe amd64 libgtk2.0-dev amd64 2.24.32-4ubuntu4 [782 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libibus-1.0-dev amd64 1.5.22-2ubuntu2.1 [179 kB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu focal/main amd64 libsys-hostname-long-perl all 1.5-1 [11.7 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu focal/main amd64 libmail-sendmail-perl all 0.80-1 [22.6 kB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopenal-dev amd64 1:1.19.1-1 [21.4 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpulse-mainloop-glib0 amd64 1:13.99.1-1ubuntu3.13 [11.7 kB]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpulse-dev amd64 1:13.99.1-1ubuntu3.13 [72.5 kB]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1-mesa-dev amd64 21.2.6-0ubuntu0.1~20.04.2 [6,420 B]\n",
            "Get:94 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsndio-dev amd64 1.5.0-3 [13.6 kB]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev-dev amd64 245.4-4ubuntu3.22 [19.7 kB]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwayland-bin amd64 1.18.0-1ubuntu0.1 [20.2 kB]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwayland-dev amd64 1.18.0-1ubuntu0.1 [64.6 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbcommon-dev amd64 0.10.0-1 [45.4 kB]\n",
            "Get:99 http://archive.ubuntu.com/ubuntu focal/main amd64 libxv-dev amd64 2:1.0.11-1 [32.5 kB]\n",
            "Get:100 http://archive.ubuntu.com/ubuntu focal/main amd64 x11proto-xf86vidmode-dev all 2019.2-1ubuntu1 [2,624 B]\n",
            "Get:101 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86vm-dev amd64 1:1.1.4-1build1 [13.3 kB]\n",
            "Get:102 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsdl2-dev amd64 2.0.10+dfsg1-3 [718 kB]\n",
            "Get:103 http://archive.ubuntu.com/ubuntu focal/universe amd64 libwildmidi-config all 0.4.3-1 [12.4 kB]\n",
            "Get:104 http://archive.ubuntu.com/ubuntu focal/universe amd64 libwildmidi2 amd64 0.4.3-1 [59.9 kB]\n",
            "Get:105 http://archive.ubuntu.com/ubuntu focal/universe amd64 libwildmidi-dev amd64 0.4.3-1 [93.2 kB]\n",
            "Get:106 http://archive.ubuntu.com/ubuntu focal/universe amd64 nasm amd64 2.14.02-1 [362 kB]\n",
            "Get:107 http://archive.ubuntu.com/ubuntu focal/universe amd64 timidity amd64 2.14.0-8build1 [613 kB]\n",
            "Fetched 169 MB in 5s (34.7 MB/s)\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 123069 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libblkid1_2.34-0.1ubuntu9.4_amd64.deb ...\r\n",
            "Unpacking libblkid1:amd64 (2.34-0.1ubuntu9.4) over (2.34-0.1ubuntu9.3) ...\r\n",
            "Setting up libblkid1:amd64 (2.34-0.1ubuntu9.4) ...\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 123069 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libmount1_2.34-0.1ubuntu9.4_amd64.deb ...\r\n",
            "Unpacking libmount1:amd64 (2.34-0.1ubuntu9.4) over (2.34-0.1ubuntu9.3) ...\r\n",
            "Setting up libmount1:amd64 (2.34-0.1ubuntu9.4) ...\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 123069 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.22_amd64.deb ...\r\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.22) over (245.4-4ubuntu3.21) ...\r\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.22) ...\r\n",
            "Selecting previously unselected package gettext-base.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 123069 files and directories currently installed.)\r\n",
            "Preparing to unpack .../000-gettext-base_0.19.8.1-10build1_amd64.deb ...\r\n",
            "Unpacking gettext-base (0.19.8.1-10build1) ...\r\n",
            "Selecting previously unselected package autopoint.\r\n",
            "Preparing to unpack .../001-autopoint_0.19.8.1-10build1_all.deb ...\r\n",
            "Unpacking autopoint (0.19.8.1-10build1) ...\r\n",
            "Selecting previously unselected package libtool.\r\n",
            "Preparing to unpack .../002-libtool_2.4.6-14_all.deb ...\r\n",
            "Unpacking libtool (2.4.6-14) ...\r\n",
            "Selecting previously unselected package dh-autoreconf.\r\n",
            "Preparing to unpack .../003-dh-autoreconf_19_all.deb ...\r\n",
            "Unpacking dh-autoreconf (19) ...\r\n",
            "Selecting previously unselected package libdebhelper-perl.\r\n",
            "Preparing to unpack .../004-libdebhelper-perl_12.10ubuntu1_all.deb ...\r\n",
            "Unpacking libdebhelper-perl (12.10ubuntu1) ...\r\n",
            "Selecting previously unselected package libarchive-zip-perl.\r\n",
            "Preparing to unpack .../005-libarchive-zip-perl_1.67-2_all.deb ...\r\n",
            "Unpacking libarchive-zip-perl (1.67-2) ...\r\n",
            "Selecting previously unselected package libsub-override-perl.\r\n",
            "Preparing to unpack .../006-libsub-override-perl_0.09-2_all.deb ...\r\n",
            "Unpacking libsub-override-perl (0.09-2) ...\r\n",
            "Selecting previously unselected package libfile-stripnondeterminism-perl.\r\n",
            "Preparing to unpack .../007-libfile-stripnondeterminism-perl_1.7.0-1_all.deb ...\r\n",
            "Unpacking libfile-stripnondeterminism-perl (1.7.0-1) ...\r\n",
            "Selecting previously unselected package dh-strip-nondeterminism.\r\n",
            "Preparing to unpack .../008-dh-strip-nondeterminism_1.7.0-1_all.deb ...\r\n",
            "Unpacking dh-strip-nondeterminism (1.7.0-1) ...\r\n",
            "Selecting previously unselected package dwz.\r\n",
            "Preparing to unpack .../009-dwz_0.13-5_amd64.deb ...\r\n",
            "Unpacking dwz (0.13-5) ...\r\n",
            "Selecting previously unselected package libcroco3:amd64.\r\n",
            "Preparing to unpack .../010-libcroco3_0.6.13-1_amd64.deb ...\r\n",
            "Unpacking libcroco3:amd64 (0.6.13-1) ...\r\n",
            "Selecting previously unselected package gettext.\r\n",
            "Preparing to unpack .../011-gettext_0.19.8.1-10build1_amd64.deb ...\r\n",
            "Unpacking gettext (0.19.8.1-10build1) ...\r\n",
            "Selecting previously unselected package intltool-debian.\r\n",
            "Preparing to unpack .../012-intltool-debian_0.35.0+20060710.5_all.deb ...\r\n",
            "Unpacking intltool-debian (0.35.0+20060710.5) ...\r\n",
            "Selecting previously unselected package po-debconf.\r\n",
            "Preparing to unpack .../013-po-debconf_1.0.21_all.deb ...\r\n",
            "Unpacking po-debconf (1.0.21) ...\r\n",
            "Selecting previously unselected package debhelper.\r\n",
            "Preparing to unpack .../014-debhelper_12.10ubuntu1_all.deb ...\r\n",
            "Unpacking debhelper (12.10ubuntu1) ...\r\n",
            "Selecting previously unselected package fluid-soundfont-gm.\r\n",
            "Preparing to unpack .../015-fluid-soundfont-gm_3.1-5.1_all.deb ...\r\n",
            "Unpacking fluid-soundfont-gm (3.1-5.1) ...\r\n",
            "Selecting previously unselected package freepats.\r\n",
            "Preparing to unpack .../016-freepats_20060219-1_all.deb ...\r\n",
            "Unpacking freepats (20060219-1) ...\r\n",
            "Selecting previously unselected package gir1.2-atk-1.0:amd64.\r\n",
            "Preparing to unpack .../017-gir1.2-atk-1.0_2.35.1-1ubuntu2_amd64.deb ...\r\n",
            "Unpacking gir1.2-atk-1.0:amd64 (2.35.1-1ubuntu2) ...\r\n",
            "Selecting previously unselected package gir1.2-freedesktop:amd64.\r\n",
            "Preparing to unpack .../018-gir1.2-freedesktop_1.64.1-1~ubuntu20.04.1_amd64.deb ...\r\n",
            "Unpacking gir1.2-freedesktop:amd64 (1.64.1-1~ubuntu20.04.1) ...\r\n",
            "Selecting previously unselected package gir1.2-gdkpixbuf-2.0:amd64.\r\n",
            "Preparing to unpack .../019-gir1.2-gdkpixbuf-2.0_2.40.0+dfsg-3ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking gir1.2-gdkpixbuf-2.0:amd64 (2.40.0+dfsg-3ubuntu0.4) ...\r\n",
            "Selecting previously unselected package libgtk2.0-common.\r\n",
            "Preparing to unpack .../020-libgtk2.0-common_2.24.32-4ubuntu4_all.deb ...\r\n",
            "Unpacking libgtk2.0-common (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package libpangoxft-1.0-0:amd64.\r\n",
            "Preparing to unpack .../021-libpangoxft-1.0-0_1.44.7-2ubuntu4_amd64.deb ...\r\n",
            "Unpacking libpangoxft-1.0-0:amd64 (1.44.7-2ubuntu4) ...\r\n",
            "Selecting previously unselected package gir1.2-pango-1.0:amd64.\r\n",
            "Preparing to unpack .../022-gir1.2-pango-1.0_1.44.7-2ubuntu4_amd64.deb ...\r\n",
            "Unpacking gir1.2-pango-1.0:amd64 (1.44.7-2ubuntu4) ...\r\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\r\n",
            "Preparing to unpack .../023-libgtk2.0-0_2.24.32-4ubuntu4_amd64.deb ...\r\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package gir1.2-gtk-2.0:amd64.\r\n",
            "Preparing to unpack .../024-gir1.2-gtk-2.0_2.24.32-4ubuntu4_amd64.deb ...\r\n",
            "Unpacking gir1.2-gtk-2.0:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package gir1.2-harfbuzz-0.0:amd64.\r\n",
            "Preparing to unpack .../025-gir1.2-harfbuzz-0.0_2.6.4-1ubuntu4.2_amd64.deb ...\r\n",
            "Unpacking gir1.2-harfbuzz-0.0:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\r\n",
            "Preparing to unpack .../026-libibus-1.0-5_1.5.22-2ubuntu2.1_amd64.deb ...\r\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.22-2ubuntu2.1) ...\r\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\r\n",
            "Preparing to unpack .../027-gir1.2-ibus-1.0_1.5.22-2ubuntu2.1_amd64.deb ...\r\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.22-2ubuntu2.1) ...\r\n",
            "Selecting previously unselected package libao-common.\r\n",
            "Preparing to unpack .../028-libao-common_1.2.2+20180113-1ubuntu1_all.deb ...\r\n",
            "Unpacking libao-common (1.2.2+20180113-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libao4:amd64.\r\n",
            "Preparing to unpack .../029-libao4_1.2.2+20180113-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libarchive-cpio-perl.\r\n",
            "Preparing to unpack .../030-libarchive-cpio-perl_0.10-1_all.deb ...\r\n",
            "Unpacking libarchive-cpio-perl (0.10-1) ...\r\n",
            "Selecting previously unselected package libffi-dev:amd64.\r\n",
            "Preparing to unpack .../031-libffi-dev_3.3-4_amd64.deb ...\r\n",
            "Unpacking libffi-dev:amd64 (3.3-4) ...\r\n",
            "Selecting previously unselected package libglib2.0-dev-bin.\r\n",
            "Preparing to unpack .../032-libglib2.0-dev-bin_2.64.6-1~ubuntu20.04.6_amd64.deb ...\r\n",
            "Unpacking libglib2.0-dev-bin (2.64.6-1~ubuntu20.04.6) ...\r\n",
            "Selecting previously unselected package libblkid-dev:amd64.\r\n",
            "Preparing to unpack .../033-libblkid-dev_2.34-0.1ubuntu9.4_amd64.deb ...\r\n",
            "Unpacking libblkid-dev:amd64 (2.34-0.1ubuntu9.4) ...\r\n",
            "Selecting previously unselected package libmount-dev:amd64.\r\n",
            "Preparing to unpack .../034-libmount-dev_2.34-0.1ubuntu9.4_amd64.deb ...\r\n",
            "Unpacking libmount-dev:amd64 (2.34-0.1ubuntu9.4) ...\r\n",
            "Selecting previously unselected package libsepol1-dev:amd64.\r\n",
            "Preparing to unpack .../035-libsepol1-dev_3.0-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libsepol1-dev:amd64 (3.0-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libselinux1-dev:amd64.\r\n",
            "Preparing to unpack .../036-libselinux1-dev_3.0-1build2_amd64.deb ...\r\n",
            "Unpacking libselinux1-dev:amd64 (3.0-1build2) ...\r\n",
            "Selecting previously unselected package libglib2.0-dev:amd64.\r\n",
            "Preparing to unpack .../037-libglib2.0-dev_2.64.6-1~ubuntu20.04.6_amd64.deb ...\r\n",
            "Unpacking libglib2.0-dev:amd64 (2.64.6-1~ubuntu20.04.6) ...\r\n",
            "Selecting previously unselected package libatk1.0-dev:amd64.\r\n",
            "Preparing to unpack .../038-libatk1.0-dev_2.35.1-1ubuntu2_amd64.deb ...\r\n",
            "Unpacking libatk1.0-dev:amd64 (2.35.1-1ubuntu2) ...\r\n",
            "Selecting previously unselected package liblzo2-2:amd64.\r\n",
            "Preparing to unpack .../039-liblzo2-2_2.10-2_amd64.deb ...\r\n",
            "Unpacking liblzo2-2:amd64 (2.10-2) ...\r\n",
            "Selecting previously unselected package libcairo-script-interpreter2:amd64.\r\n",
            "Preparing to unpack .../040-libcairo-script-interpreter2_1.16.0-4ubuntu1_amd64.deb ...\r\n",
            "Unpacking libcairo-script-interpreter2:amd64 (1.16.0-4ubuntu1) ...\r\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\r\n",
            "Preparing to unpack .../041-libpixman-1-dev_0.38.4-0ubuntu2.1_amd64.deb ...\r\n",
            "Unpacking libpixman-1-dev:amd64 (0.38.4-0ubuntu2.1) ...\r\n",
            "Selecting previously unselected package libxcb-render0-dev:amd64.\r\n",
            "Preparing to unpack .../042-libxcb-render0-dev_1.14-2_amd64.deb ...\r\n",
            "Unpacking libxcb-render0-dev:amd64 (1.14-2) ...\r\n",
            "Selecting previously unselected package libxcb-shm0-dev:amd64.\r\n",
            "Preparing to unpack .../043-libxcb-shm0-dev_1.14-2_amd64.deb ...\r\n",
            "Unpacking libxcb-shm0-dev:amd64 (1.14-2) ...\r\n",
            "Selecting previously unselected package libcairo2-dev:amd64.\r\n",
            "Preparing to unpack .../044-libcairo2-dev_1.16.0-4ubuntu1_amd64.deb ...\r\n",
            "Unpacking libcairo2-dev:amd64 (1.16.0-4ubuntu1) ...\r\n",
            "Selecting previously unselected package libdatrie-dev:amd64.\r\n",
            "Preparing to unpack .../045-libdatrie-dev_0.2.12-3_amd64.deb ...\r\n",
            "Unpacking libdatrie-dev:amd64 (0.2.12-3) ...\r\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\r\n",
            "Preparing to unpack .../046-libdbus-1-dev_1.12.16-2ubuntu2.3_amd64.deb ...\r\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.16-2ubuntu2.3) ...\r\n",
            "Selecting previously unselected package libgles1:amd64.\r\n",
            "Preparing to unpack .../047-libgles1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
            "Unpacking libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Selecting previously unselected package libgles-dev:amd64.\r\n",
            "Preparing to unpack .../048-libgles-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
            "Unpacking libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Selecting previously unselected package libopengl-dev:amd64.\r\n",
            "Preparing to unpack .../049-libopengl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
            "Unpacking libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\r\n",
            "Preparing to unpack .../050-libglvnd-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
            "Unpacking libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Selecting previously unselected package libegl1-mesa-dev:amd64.\r\n",
            "Preparing to unpack .../051-libegl1-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
            "Unpacking libegl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\r\n",
            "Preparing to unpack .../052-libinstpatch-1.0-2_1.1.2-2build1_amd64.deb ...\r\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.2-2build1) ...\r\n",
            "Selecting previously unselected package timgm6mb-soundfont.\r\n",
            "Preparing to unpack .../053-timgm6mb-soundfont_1.3-3_all.deb ...\r\n",
            "Unpacking timgm6mb-soundfont (1.3-3) ...\r\n",
            "Selecting previously unselected package libfluidsynth2:amd64.\r\n",
            "Preparing to unpack .../054-libfluidsynth2_2.1.1-2_amd64.deb ...\r\n",
            "Unpacking libfluidsynth2:amd64 (2.1.1-2) ...\r\n",
            "Selecting previously unselected package libfluidsynth-dev:amd64.\r\n",
            "Preparing to unpack .../055-libfluidsynth-dev_2.1.1-2_amd64.deb ...\r\n",
            "Unpacking libfluidsynth-dev:amd64 (2.1.1-2) ...\r\n",
            "Selecting previously unselected package libfribidi-dev:amd64.\r\n",
            "Preparing to unpack .../056-libfribidi-dev_1.0.8-2ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libfribidi-dev:amd64 (1.0.8-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libgail18:amd64.\r\n",
            "Preparing to unpack .../057-libgail18_2.24.32-4ubuntu4_amd64.deb ...\r\n",
            "Unpacking libgail18:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package libgail-common:amd64.\r\n",
            "Preparing to unpack .../058-libgail-common_2.24.32-4ubuntu4_amd64.deb ...\r\n",
            "Unpacking libgail-common:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package libgdk-pixbuf2.0-bin.\r\n",
            "Preparing to unpack .../059-libgdk-pixbuf2.0-bin_2.40.0+dfsg-3ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking libgdk-pixbuf2.0-bin (2.40.0+dfsg-3ubuntu0.4) ...\r\n",
            "Selecting previously unselected package libgdk-pixbuf2.0-dev:amd64.\r\n",
            "Preparing to unpack .../060-libgdk-pixbuf2.0-dev_2.40.0+dfsg-3ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking libgdk-pixbuf2.0-dev:amd64 (2.40.0+dfsg-3ubuntu0.4) ...\r\n",
            "Selecting previously unselected package libgles2-mesa-dev:amd64.\r\n",
            "Preparing to unpack .../061-libgles2-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
            "Unpacking libgles2-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
            "Selecting previously unselected package libgme-dev:amd64.\r\n",
            "Preparing to unpack .../062-libgme-dev_0.6.2-1build1_amd64.deb ...\r\n",
            "Unpacking libgme-dev:amd64 (0.6.2-1build1) ...\r\n",
            "Selecting previously unselected package libgraphite2-dev:amd64.\r\n",
            "Preparing to unpack .../063-libgraphite2-dev_1.3.13-11build1_amd64.deb ...\r\n",
            "Unpacking libgraphite2-dev:amd64 (1.3.13-11build1) ...\r\n",
            "Selecting previously unselected package libgtk2.0-bin.\r\n",
            "Preparing to unpack .../064-libgtk2.0-bin_2.24.32-4ubuntu4_amd64.deb ...\r\n",
            "Unpacking libgtk2.0-bin (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package libharfbuzz-icu0:amd64.\r\n",
            "Preparing to unpack .../065-libharfbuzz-icu0_2.6.4-1ubuntu4.2_amd64.deb ...\r\n",
            "Unpacking libharfbuzz-icu0:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Selecting previously unselected package libharfbuzz-gobject0:amd64.\r\n",
            "Preparing to unpack .../066-libharfbuzz-gobject0_2.6.4-1ubuntu4.2_amd64.deb ...\r\n",
            "Unpacking libharfbuzz-gobject0:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Selecting previously unselected package libharfbuzz-dev:amd64.\r\n",
            "Preparing to unpack .../067-libharfbuzz-dev_2.6.4-1ubuntu4.2_amd64.deb ...\r\n",
            "Unpacking libharfbuzz-dev:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Selecting previously unselected package libthai-dev:amd64.\r\n",
            "Preparing to unpack .../068-libthai-dev_0.1.28-3_amd64.deb ...\r\n",
            "Unpacking libthai-dev:amd64 (0.1.28-3) ...\r\n",
            "Selecting previously unselected package pango1.0-tools.\r\n",
            "Preparing to unpack .../069-pango1.0-tools_1.44.7-2ubuntu4_amd64.deb ...\r\n",
            "Unpacking pango1.0-tools (1.44.7-2ubuntu4) ...\r\n",
            "Selecting previously unselected package libpango1.0-dev:amd64.\r\n",
            "Preparing to unpack .../070-libpango1.0-dev_1.44.7-2ubuntu4_amd64.deb ...\r\n",
            "Unpacking libpango1.0-dev:amd64 (1.44.7-2ubuntu4) ...\r\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\r\n",
            "Preparing to unpack .../071-x11proto-xinerama-dev_2019.2-1ubuntu1_all.deb ...\r\n",
            "Unpacking x11proto-xinerama-dev (2019.2-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\r\n",
            "Preparing to unpack .../072-libxinerama-dev_2%3a1.1.4-2_amd64.deb ...\r\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.4-2) ...\r\n",
            "Selecting previously unselected package libxfixes-dev:amd64.\r\n",
            "Preparing to unpack .../073-libxfixes-dev_1%3a5.0.3-2_amd64.deb ...\r\n",
            "Unpacking libxfixes-dev:amd64 (1:5.0.3-2) ...\r\n",
            "Selecting previously unselected package x11proto-input-dev.\r\n",
            "Preparing to unpack .../074-x11proto-input-dev_2019.2-1ubuntu1_all.deb ...\r\n",
            "Unpacking x11proto-input-dev (2019.2-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libxi-dev:amd64.\r\n",
            "Preparing to unpack .../075-libxi-dev_2%3a1.7.10-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libxi-dev:amd64 (2:1.7.10-0ubuntu1) ...\r\n",
            "Selecting previously unselected package x11proto-randr-dev.\r\n",
            "Preparing to unpack .../076-x11proto-randr-dev_2019.2-1ubuntu1_all.deb ...\r\n",
            "Unpacking x11proto-randr-dev (2019.2-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\r\n",
            "Preparing to unpack .../077-libxrandr-dev_2%3a1.5.2-0ubuntu1_amd64.deb ...\r\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.2-0ubuntu1) ...\r\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\r\n",
            "Preparing to unpack .../078-libxcursor-dev_1%3a1.2.0-2_amd64.deb ...\r\n",
            "Unpacking libxcursor-dev:amd64 (1:1.2.0-2) ...\r\n",
            "Selecting previously unselected package libxcomposite-dev:amd64.\r\n",
            "Preparing to unpack .../079-libxcomposite-dev_1%3a0.4.5-1_amd64.deb ...\r\n",
            "Unpacking libxcomposite-dev:amd64 (1:0.4.5-1) ...\r\n",
            "Selecting previously unselected package libxdamage-dev:amd64.\r\n",
            "Preparing to unpack .../080-libxdamage-dev_1%3a1.1.5-2_amd64.deb ...\r\n",
            "Unpacking libxdamage-dev:amd64 (1:1.1.5-2) ...\r\n",
            "Selecting previously unselected package libxml2-utils.\r\n",
            "Preparing to unpack .../081-libxml2-utils_2.9.10+dfsg-5ubuntu0.20.04.6_amd64.deb ...\r\n",
            "Unpacking libxml2-utils (2.9.10+dfsg-5ubuntu0.20.04.6) ...\r\n",
            "Selecting previously unselected package libgtk2.0-dev:amd64.\r\n",
            "Preparing to unpack .../082-libgtk2.0-dev_2.24.32-4ubuntu4_amd64.deb ...\r\n",
            "Unpacking libgtk2.0-dev:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\r\n",
            "Preparing to unpack .../083-libibus-1.0-dev_1.5.22-2ubuntu2.1_amd64.deb ...\r\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.22-2ubuntu2.1) ...\r\n",
            "Selecting previously unselected package libsys-hostname-long-perl.\r\n",
            "Preparing to unpack .../084-libsys-hostname-long-perl_1.5-1_all.deb ...\r\n",
            "Unpacking libsys-hostname-long-perl (1.5-1) ...\r\n",
            "Selecting previously unselected package libmail-sendmail-perl.\r\n",
            "Preparing to unpack .../085-libmail-sendmail-perl_0.80-1_all.deb ...\r\n",
            "Unpacking libmail-sendmail-perl (0.80-1) ...\r\n",
            "Selecting previously unselected package libopenal-dev:amd64.\r\n",
            "Preparing to unpack .../086-libopenal-dev_1%3a1.19.1-1_amd64.deb ...\r\n",
            "Unpacking libopenal-dev:amd64 (1:1.19.1-1) ...\r\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\r\n",
            "Preparing to unpack .../087-libpulse-mainloop-glib0_1%3a13.99.1-1ubuntu3.13_amd64.deb ...\r\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:13.99.1-1ubuntu3.13) ...\r\n",
            "Selecting previously unselected package libpulse-dev:amd64.\r\n",
            "Preparing to unpack .../088-libpulse-dev_1%3a13.99.1-1ubuntu3.13_amd64.deb ...\r\n",
            "Unpacking libpulse-dev:amd64 (1:13.99.1-1ubuntu3.13) ...\r\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\r\n",
            "Preparing to unpack .../089-libgl1-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
            "Unpacking libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
            "Selecting previously unselected package libsndio-dev:amd64.\r\n",
            "Preparing to unpack .../090-libsndio-dev_1.5.0-3_amd64.deb ...\r\n",
            "Unpacking libsndio-dev:amd64 (1.5.0-3) ...\r\n",
            "Selecting previously unselected package libudev-dev:amd64.\r\n",
            "Preparing to unpack .../091-libudev-dev_245.4-4ubuntu3.22_amd64.deb ...\r\n",
            "Unpacking libudev-dev:amd64 (245.4-4ubuntu3.22) ...\r\n",
            "Selecting previously unselected package libwayland-bin.\r\n",
            "Preparing to unpack .../092-libwayland-bin_1.18.0-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libwayland-bin (1.18.0-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libwayland-dev:amd64.\r\n",
            "Preparing to unpack .../093-libwayland-dev_1.18.0-1ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking libwayland-dev:amd64 (1.18.0-1ubuntu0.1) ...\r\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\r\n",
            "Preparing to unpack .../094-libxkbcommon-dev_0.10.0-1_amd64.deb ...\r\n",
            "Unpacking libxkbcommon-dev:amd64 (0.10.0-1) ...\r\n",
            "Selecting previously unselected package libxv-dev:amd64.\r\n",
            "Preparing to unpack .../095-libxv-dev_2%3a1.0.11-1_amd64.deb ...\r\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1) ...\r\n",
            "Selecting previously unselected package x11proto-xf86vidmode-dev.\r\n",
            "Preparing to unpack .../096-x11proto-xf86vidmode-dev_2019.2-1ubuntu1_all.deb ...\r\n",
            "Unpacking x11proto-xf86vidmode-dev (2019.2-1ubuntu1) ...\r\n",
            "Selecting previously unselected package libxxf86vm-dev:amd64.\r\n",
            "Preparing to unpack .../097-libxxf86vm-dev_1%3a1.1.4-1build1_amd64.deb ...\r\n",
            "Unpacking libxxf86vm-dev:amd64 (1:1.1.4-1build1) ...\r\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\r\n",
            "Preparing to unpack .../098-libsdl2-dev_2.0.10+dfsg1-3_amd64.deb ...\r\n",
            "Unpacking libsdl2-dev:amd64 (2.0.10+dfsg1-3) ...\r\n",
            "Selecting previously unselected package libwildmidi-config.\r\n",
            "Preparing to unpack .../099-libwildmidi-config_0.4.3-1_all.deb ...\r\n",
            "Unpacking libwildmidi-config (0.4.3-1) ...\r\n",
            "Selecting previously unselected package libwildmidi2:amd64.\r\n",
            "Preparing to unpack .../100-libwildmidi2_0.4.3-1_amd64.deb ...\r\n",
            "Unpacking libwildmidi2:amd64 (0.4.3-1) ...\r\n",
            "Selecting previously unselected package libwildmidi-dev.\r\n",
            "Preparing to unpack .../101-libwildmidi-dev_0.4.3-1_amd64.deb ...\r\n",
            "Unpacking libwildmidi-dev (0.4.3-1) ...\r\n",
            "Selecting previously unselected package nasm.\r\n",
            "Preparing to unpack .../102-nasm_2.14.02-1_amd64.deb ...\r\n",
            "Unpacking nasm (2.14.02-1) ...\r\n",
            "Selecting previously unselected package timidity.\r\n",
            "Preparing to unpack .../103-timidity_2.14.0-8build1_amd64.deb ...\r\n",
            "Unpacking timidity (2.14.0-8build1) ...\r\n",
            "Setting up libglib2.0-dev-bin (2.64.6-1~ubuntu20.04.6) ...\r\n",
            "Setting up libblkid-dev:amd64 (2.34-0.1ubuntu9.4) ...\r\n",
            "Setting up freepats (20060219-1) ...\r\n",
            "Setting up gir1.2-freedesktop:amd64 (1.64.1-1~ubuntu20.04.1) ...\r\n",
            "Setting up libsndio-dev:amd64 (1.5.0-3) ...\r\n",
            "Setting up libharfbuzz-icu0:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Setting up libpixman-1-dev:amd64 (0.38.4-0ubuntu2.1) ...\r\n",
            "Setting up libpangoxft-1.0-0:amd64 (1.44.7-2ubuntu4) ...\r\n",
            "Setting up gir1.2-gdkpixbuf-2.0:amd64 (2.40.0+dfsg-3ubuntu0.4) ...\r\n",
            "Setting up libtool (2.4.6-14) ...\r\n",
            "Setting up gir1.2-atk-1.0:amd64 (2.35.1-1ubuntu2) ...\r\n",
            "Setting up libarchive-zip-perl (1.67-2) ...\r\n",
            "Setting up libopenal-dev:amd64 (1:1.19.1-1) ...\r\n",
            "Setting up x11proto-randr-dev (2019.2-1ubuntu1) ...\r\n",
            "Setting up libfribidi-dev:amd64 (1.0.8-2ubuntu0.1) ...\r\n",
            "Setting up libxkbcommon-dev:amd64 (0.10.0-1) ...\r\n",
            "Setting up libdebhelper-perl (12.10ubuntu1) ...\r\n",
            "Setting up libibus-1.0-5:amd64 (1.5.22-2ubuntu2.1) ...\r\n",
            "Setting up pango1.0-tools (1.44.7-2ubuntu4) ...\r\n",
            "Setting up libsepol1-dev:amd64 (3.0-1ubuntu0.1) ...\r\n",
            "Setting up libgme-dev:amd64 (0.6.2-1build1) ...\r\n",
            "Setting up gettext-base (0.19.8.1-10build1) ...\r\n",
            "Setting up liblzo2-2:amd64 (2.10-2) ...\r\n",
            "Setting up libharfbuzz-gobject0:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Setting up libffi-dev:amd64 (3.3-4) ...\r\n",
            "Setting up gir1.2-harfbuzz-0.0:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Setting up x11proto-xinerama-dev (2019.2-1ubuntu1) ...\r\n",
            "Setting up libao-common (1.2.2+20180113-1ubuntu1) ...\r\n",
            "Setting up libxfixes-dev:amd64 (1:5.0.3-2) ...\r\n",
            "Setting up libxcb-shm0-dev:amd64 (1.14-2) ...\r\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1) ...\r\n",
            "Setting up libwayland-bin (1.18.0-1ubuntu0.1) ...\r\n",
            "Setting up libgraphite2-dev:amd64 (1.3.13-11build1) ...\r\n",
            "Setting up gir1.2-pango-1.0:amd64 (1.44.7-2ubuntu4) ...\r\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.2-0ubuntu1) ...\r\n",
            "Setting up libwildmidi-config (0.4.3-1) ...\r\n",
            "Setting up libdbus-1-dev:amd64 (1.12.16-2ubuntu2.3) ...\r\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:13.99.1-1ubuntu3.13) ...\r\n",
            "Setting up libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Setting up x11proto-input-dev (2019.2-1ubuntu1) ...\r\n",
            "Setting up autopoint (0.19.8.1-10build1) ...\r\n",
            "Setting up libcroco3:amd64 (0.6.13-1) ...\r\n",
            "Setting up libudev-dev:amd64 (245.4-4ubuntu3.22) ...\r\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.4-2) ...\r\n",
            "Setting up libxcb-render0-dev:amd64 (1.14-2) ...\r\n",
            "Setting up nasm (2.14.02-1) ...\r\n",
            "Setting up x11proto-xf86vidmode-dev (2019.2-1ubuntu1) ...\r\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1ubuntu1) ...\r\n",
            "Setting up dwz (0.13-5) ...\r\n",
            "Setting up libdatrie-dev:amd64 (0.2.12-3) ...\r\n",
            "Setting up libwildmidi2:amd64 (0.4.3-1) ...\r\n",
            "Setting up libgdk-pixbuf2.0-bin (2.40.0+dfsg-3ubuntu0.4) ...\r\n",
            "Setting up libarchive-cpio-perl (0.10-1) ...\r\n",
            "Setting up libsub-override-perl (0.09-2) ...\r\n",
            "Setting up fluid-soundfont-gm (3.1-5.1) ...\r\n",
            "Setting up libgtk2.0-common (2.24.32-4ubuntu4) ...\r\n",
            "Setting up libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Setting up libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Setting up libxi-dev:amd64 (2:1.7.10-0ubuntu1) ...\r\n",
            "Setting up libxml2-utils (2.9.10+dfsg-5ubuntu0.20.04.6) ...\r\n",
            "Setting up timgm6mb-soundfont (1.3-3) ...\r\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\r\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\r\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.22-2ubuntu2.1) ...\r\n",
            "Setting up libsys-hostname-long-perl (1.5-1) ...\r\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.2-2build1) ...\r\n",
            "Setting up libmount-dev:amd64 (2.34-0.1ubuntu9.4) ...\r\n",
            "Setting up libxdamage-dev:amd64 (1:1.1.5-2) ...\r\n",
            "Setting up libfile-stripnondeterminism-perl (1.7.0-1) ...\r\n",
            "Setting up libcairo-script-interpreter2:amd64 (1.16.0-4ubuntu1) ...\r\n",
            "Setting up gettext (0.19.8.1-10build1) ...\r\n",
            "Setting up timidity (2.14.0-8build1) ...\r\n",
            "Setting up libselinux1-dev:amd64 (3.0-1build2) ...\r\n",
            "Setting up libxcomposite-dev:amd64 (1:0.4.5-1) ...\r\n",
            "Setting up libxcursor-dev:amd64 (1:1.2.0-2) ...\r\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Setting up libxxf86vm-dev:amd64 (1:1.1.4-1build1) ...\r\n",
            "Setting up libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
            "Setting up libwildmidi-dev (0.4.3-1) ...\r\n",
            "Setting up libwayland-dev:amd64 (1.18.0-1ubuntu0.1) ...\r\n",
            "Setting up intltool-debian (0.35.0+20060710.5) ...\r\n",
            "Setting up libmail-sendmail-perl (0.80-1) ...\r\n",
            "Setting up libglib2.0-dev:amd64 (2.64.6-1~ubuntu20.04.6) ...\r\n",
            "Setting up gir1.2-gtk-2.0:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Setting up libthai-dev:amd64 (0.1.28-3) ...\r\n",
            "Setting up dh-strip-nondeterminism (1.7.0-1) ...\r\n",
            "Setting up libgail18:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Setting up libgtk2.0-bin (2.24.32-4ubuntu4) ...\r\n",
            "Setting up libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
            "Setting up libfluidsynth2:amd64 (2.1.1-2) ...\r\n",
            "Setting up libegl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
            "Setting up libgles2-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
            "Setting up libgail-common:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Setting up po-debconf (1.0.21) ...\r\n",
            "Setting up libfluidsynth-dev:amd64 (2.1.1-2) ...\r\n",
            "Setting up dh-autoreconf (19) ...\r\n",
            "Setting up debhelper (12.10ubuntu1) ...\r\n",
            "Processing triggers for man-db (2.9.1-1) ...\r\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\r\n",
            "Processing triggers for libglib2.0-0:amd64 (2.64.6-1~ubuntu20.04.6) ...\r\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.22-2ubuntu2.1) ...\r\n",
            "Setting up libgdk-pixbuf2.0-dev:amd64 (2.40.0+dfsg-3ubuntu0.4) ...\r\n",
            "Setting up libpulse-dev:amd64 (1:13.99.1-1ubuntu3.13) ...\r\n",
            "Setting up libcairo2-dev:amd64 (1.16.0-4ubuntu1) ...\r\n",
            "Setting up libatk1.0-dev:amd64 (2.35.1-1ubuntu2) ...\r\n",
            "Setting up libharfbuzz-dev:amd64 (2.6.4-1ubuntu4.2) ...\r\n",
            "Setting up libsdl2-dev:amd64 (2.0.10+dfsg1-3) ...\r\n",
            "Setting up libpango1.0-dev:amd64 (1.44.7-2ubuntu4) ...\r\n",
            "Setting up libgtk2.0-dev:amd64 (2.24.32-4ubuntu4) ...\r\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libboost-all-dev is already the newest version (1.71.0.0ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  liblua5.1-0 libtool-bin\n",
            "The following NEW packages will be installed:\n",
            "  liblua5.1-0 liblua5.1-0-dev libtool-bin\n",
            "0 upgraded, 3 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 302 kB of archives.\n",
            "After this operation, 1,631 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblua5.1-0 amd64 5.1.5-8.1build4 [99.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblua5.1-0-dev amd64 5.1.5-8.1build4 [122 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libtool-bin amd64 2.4.6-14 [80.1 kB]\n",
            "Fetched 302 kB in 1s (348 kB/s)\n",
            "Selecting previously unselected package liblua5.1-0:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126563 files and directories currently installed.)\r\n",
            "Preparing to unpack .../liblua5.1-0_5.1.5-8.1build4_amd64.deb ...\r\n",
            "Unpacking liblua5.1-0:amd64 (5.1.5-8.1build4) ...\r\n",
            "Selecting previously unselected package liblua5.1-0-dev:amd64.\r\n",
            "Preparing to unpack .../liblua5.1-0-dev_5.1.5-8.1build4_amd64.deb ...\r\n",
            "Unpacking liblua5.1-0-dev:amd64 (5.1.5-8.1build4) ...\r\n",
            "Selecting previously unselected package libtool-bin.\r\n",
            "Preparing to unpack .../libtool-bin_2.4.6-14_amd64.deb ...\r\n",
            "Unpacking libtool-bin (2.4.6-14) ...\r\n",
            "Setting up libtool-bin (2.4.6-14) ...\r\n",
            "Setting up liblua5.1-0:amd64 (5.1.5-8.1build4) ...\r\n",
            "Setting up liblua5.1-0-dev:amd64 (5.1.5-8.1build4) ...\r\n",
            "Processing triggers for man-db (2.9.1-1) ...\r\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Install deps from\n",
        "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
        "\n",
        "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
        "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
        "libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "apt-get install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "apt-get install liblua5.1-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa4giPWrLKJZ"
      },
      "source": [
        "## Install vizdoom, tensorflow and scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hr_2wQnn_LN",
        "outputId": "9cac3ef2-8857-421a-c162-64c078e1c4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vizdoom\n",
            "  Downloading vizdoom-1.2.0.tar.gz (15.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vizdoom) (1.22.4)\n",
            "Collecting gymnasium>=0.28.0 (from vizdoom)\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from vizdoom) (2.3.0)\n",
            "Collecting jax-jumpy>=1.0.0 (from gymnasium>=0.28.0->vizdoom)\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->vizdoom) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->vizdoom)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: vizdoom\n",
            "  Building wheel for vizdoom (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vizdoom: filename=vizdoom-1.2.0-cp310-cp310-linux_x86_64.whl size=14295977 sha256=258368c0b4f214aa358eb13f036d68522dd95218d2955e10d4f367b3cd2f5f17\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/6b/8c/5539439c888350cfe7098fea10d972336cb9f0c0c132ff0073\n",
            "Successfully built vizdoom\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium, vizdoom\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 vizdoom-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install vizdoom\n",
        "%pip install tensorflow\n",
        "%pip install scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1O533Kjy13"
      },
      "source": [
        "## Step 1: Import the libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p46soGNyjy15"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf      # Deep Learning library\n",
        "import numpy as np           # Handle matrices\n",
        "from vizdoom import *        # Doom Environment\n",
        "\n",
        "import random                # Handling random number generation\n",
        "import time                  # Handling time calculation\n",
        "from skimage import transform# Help us to preprocess the frames\n",
        "\n",
        "from collections import deque# Ordered collection with ends\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13_P_Cr1jy16"
      },
      "source": [
        "## Step 2: Create our environment üéÆ\n",
        "- Now that we imported the libraries/dependencies, we will create our environment.\n",
        "- Doom environment takes:\n",
        "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
        "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
        "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out.\n",
        "\n",
        "### Our environment\n",
        "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
        "\n",
        "- A monster is spawned **randomly somewhere along the opposite wall**.\n",
        "- Player can only go **left/right and shoot**.\n",
        "- 1 hit is enough **to kill the monster**.\n",
        "- Episode finishes when **monster is killed or on timeout (300)**.\n",
        "<br><br>\n",
        "REWARDS:\n",
        "\n",
        "- +101 for killing the monster\n",
        "- -5 for missing\n",
        "- Episode ends after killing the monster or on timeout.\n",
        "- living reward = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pTJOxD5Ljy16"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Here we create our environment\n",
        "\"\"\"\n",
        "def create_environment():\n",
        "    game = vizdoom.DoomGame()\n",
        "\n",
        "    # Load the correct configuration\n",
        "    game.load_config(\"/content/drive/MyDrive/Doom/basic.cfg\")\n",
        "\n",
        "    # Load the correct scenario (in our case basic scenario)\n",
        "    game.set_doom_scenario_path(\"/content/drive/MyDrive/Doom/basic.wad\")\n",
        "\n",
        "    # For Google Collab\n",
        "    game.set_window_visible(False)\n",
        "\n",
        "    game.init()\n",
        "\n",
        "    # Here our possible actions\n",
        "    left = [1, 0, 0]\n",
        "    right = [0, 1, 0]\n",
        "    shoot = [0, 0, 1]\n",
        "    possible_actions = [left, right, shoot]\n",
        "\n",
        "    return game, possible_actions\n",
        "\n",
        "\"\"\"\n",
        "Here we performing random action to test the environment\n",
        "\"\"\"\n",
        "def test_environment():\n",
        "    game = vizdoom.DoomGame()\n",
        "    game.load_config(\"/content/drive/MyDrive/Doom/basic.cfg\")\n",
        "    game.set_doom_scenario_path(\"/content/drive/MyDrive/Doom/basic.wad\")\n",
        "    game.init()\n",
        "    shoot = [0, 0, 1]\n",
        "    left = [1, 0, 0]\n",
        "    right = [0, 1, 0]\n",
        "    actions = [shoot, left, right]\n",
        "\n",
        "    episodes = 10\n",
        "    for i in range(episodes):\n",
        "        game.new_episode()\n",
        "        while not game.is_episode_finished():\n",
        "            state = game.get_state()\n",
        "            img = state.screen_buffer\n",
        "            misc = state.game_variables\n",
        "            action = random.choice(actions)\n",
        "            print(action)\n",
        "            reward = game.make_action(action)\n",
        "            print (\"\\treward:\", reward)\n",
        "            time.sleep(0.02)\n",
        "        print (\"Result:\", game.get_total_reward())\n",
        "        time.sleep(2)\n",
        "    game.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "game, possible_actions = create_environment()"
      ],
      "metadata": {
        "id": "bnHcmNDjBimQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xms65Qmjy17"
      },
      "source": [
        "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
        "### preprocess_frame\n",
        "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
        "<br><br>\n",
        "Our steps:\n",
        "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
        "- Crop the screen (in our case we remove the roof because it contains no information)\n",
        "- We normalize pixel values\n",
        "- Finally we resize the preprocessed frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bTd0FuIdjy18"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    preprocess_frame:\n",
        "    Take a frame.\n",
        "    Resize it.\n",
        "        __________________\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |                 |\n",
        "        |_________________|\n",
        "\n",
        "        to\n",
        "        _____________\n",
        "        |            |\n",
        "        |            |\n",
        "        |            |\n",
        "        |____________|\n",
        "    Normalize it.\n",
        "\n",
        "    return preprocessed_frame\n",
        "\n",
        "    \"\"\"\n",
        "def preprocess_frame(frame):\n",
        "    # Greyscale frame already done in our vizdoom config\n",
        "    # x = np.mean(frame,-1)\n",
        "\n",
        "    # Crop the screen (remove the roof because it contains no information)\n",
        "    cropped_frame = frame[30:-10,30:-30]\n",
        "\n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = cropped_frame/255.0\n",
        "\n",
        "    # Resize\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
        "\n",
        "    return preprocessed_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEoJcvmRjy18"
      },
      "source": [
        "### stack_frames\n",
        "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
        "\n",
        "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
        "\n",
        "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
        "\n",
        "- First we preprocess frame\n",
        "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
        "- Finally we **build the stacked state**\n",
        "\n",
        "This is how work stack:\n",
        "- For the first frame, we feed 4 frames\n",
        "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
        "- And so on\n",
        "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
        "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SIRVLOfWjy18"
      },
      "outputs": [],
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "\n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "\n",
        "    return stacked_state, stacked_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Wr2Qcxjy19"
      },
      "source": [
        "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
        "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
        "\n",
        "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
        "- Then, you'll add the training hyperparameters when you implement the training algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Oehg3_Gwjy19"
      },
      "outputs": [],
      "source": [
        "### MODEL HYPERPARAMETERS\n",
        "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels)\n",
        "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
        "learning_rate =  0.0005      # Alpha (aka learning rate)\n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "total_episodes = 1000        # Total episodes for training\n",
        "max_steps = 50              # Max possible steps in an episode\n",
        "batch_size = 32\n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability\n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Q learning hyperparameters\n",
        "gamma = 0.95               # Discounting rate\n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000          # Number of experiences the Memory can keep\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = True\n",
        "\n",
        "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
        "episode_render = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM_mJtzYjy1-"
      },
      "source": [
        "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
        "<img src=\"assets/model.png\" alt=\"Model\" />\n",
        "This is our Deep Q-learning model:\n",
        "- We take a stack of 4 frames as input\n",
        "- It passes through 3 convnets\n",
        "- Then it is flatened\n",
        "- Finally it passes through 2 FC layers\n",
        "- It outputs a Q value for each actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ohiE6DlKjy1-"
      },
      "outputs": [],
      "source": [
        "class DQNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        with tf.compat.v1.variable_scope(name):\n",
        "            # We create the placeholders\n",
        "            # *state_size means that we take each element of state_size in tuple hence it's like writing\n",
        "            # [None, 84, 84, 4]\n",
        "            self.inputs_ = tf.compat.v1.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "            self.actions_ = tf.compat.v1.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
        "\n",
        "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
        "            self.target_Q = tf.compat.v1.placeholder(tf.float32, [None], name=\"target\")\n",
        "\n",
        "            \"\"\"\n",
        "            First convnet:\n",
        "            CNN\n",
        "            BatchNormalization\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            # Input is 84x84x4\n",
        "            self.conv1 = tf.compat.v1.layers.conv2d(inputs=self.inputs_,\n",
        "                                                    filters=32,\n",
        "                                                    kernel_size=[4, 4],\n",
        "                                                    strides=[2, 2],\n",
        "                                                    padding=\"VALID\",\n",
        "                                                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
        "                                                    name=\"conv1\")\n",
        "\n",
        "            self.conv1_batchnorm = tf.compat.v1.layers.batch_normalization(self.conv1,\n",
        "                                                                          training=True,\n",
        "                                                                          epsilon=1e-5,\n",
        "                                                                          name='batch_norm1')\n",
        "\n",
        "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
        "            ## --> [20, 20, 32]\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            Second convnet:\n",
        "            CNN\n",
        "            BatchNormalization\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv2 = tf.compat.v1.layers.conv2d(inputs=self.conv1_out,\n",
        "                                                    filters=64,\n",
        "                                                    kernel_size=[4, 4],\n",
        "                                                    strides=[2, 2],\n",
        "                                                    padding=\"VALID\",\n",
        "                                                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
        "                                                    name=\"conv2\")\n",
        "\n",
        "            self.conv2_batchnorm = tf.compat.v1.layers.batch_normalization(self.conv2,\n",
        "                                                                          training=True,\n",
        "                                                                          epsilon=1e-5,\n",
        "                                                                          name='batch_norm2')\n",
        "\n",
        "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
        "            ## --> [9, 9, 64]\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            Third convnet:\n",
        "            CNN\n",
        "            BatchNormalization\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv3 = tf.compat.v1.layers.conv2d(inputs=self.conv2_out,\n",
        "                                                    filters=128,\n",
        "                                                    kernel_size=[3, 3],  # Adjusted kernel size\n",
        "                                                    strides=[1, 1],  # Adjusted strides\n",
        "                                                    padding=\"SAME\",  # Changed padding to \"SAME\"\n",
        "                                                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
        "                                                    name=\"conv3\")\n",
        "\n",
        "            self.conv3_batchnorm = tf.compat.v1.layers.batch_normalization(self.conv3,\n",
        "                                                                          training=True,\n",
        "                                                                          epsilon=1e-5,\n",
        "                                                                          name='batch_norm3')\n",
        "\n",
        "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
        "            ## --> [9, 9, 128]\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            Fourth convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv4 = tf.compat.v1.layers.conv2d(inputs=self.conv3_out,\n",
        "                                                    filters=256,\n",
        "                                                    kernel_size=[4, 4],\n",
        "                                                    strides=[2, 2],\n",
        "                                                    padding=\"VALID\",\n",
        "                                                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
        "                                                    name=\"conv4\")\n",
        "\n",
        "            self.conv4_out = tf.nn.elu(self.conv4, name=\"conv4_out\")\n",
        "            ## --> [3, 3, 256]\n",
        "\n",
        "\n",
        "            self.flatten = tf.compat.v1.layers.flatten(self.conv4_out)\n",
        "            ## --> [2304]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            self.fc = tf.compat.v1.layers.dense(inputs = self.flatten,\n",
        "                                  units = 512,\n",
        "                                  activation = tf.nn.elu,\n",
        "                                  kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
        "                                  name=\"fc1\")\n",
        "\n",
        "\n",
        "            self.output = tf.compat.v1.layers.dense(inputs = self.fc,\n",
        "                                          kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
        "                                          units = 3,\n",
        "                                          activation=None)\n",
        "\n",
        "\n",
        "            # Q is our predicted Q value.\n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
        "\n",
        "\n",
        "            # The loss is the difference between our predicted Q_values and the Q_target\n",
        "            # Sum(Qtarget - Q)^2\n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "\n",
        "            self.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "equMspOFjy1_"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BhBN0iVQjy1_",
        "outputId": "53679ef3-ebcc-40ed-eead-2ae1a0215452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "# Reset the graph\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Instantiate the DQNetwork\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_anr1wgjy2A"
      },
      "source": [
        "## Step 6: Experience Replay üîÅ\n",
        "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
        "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
        "\n",
        "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p3aX1W82jy2A"
      },
      "outputs": [],
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size),\n",
        "                                size = batch_size,\n",
        "                                replace = False)\n",
        "\n",
        "        return [self.buffer[i] for i in index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCTmVC-jy2B"
      },
      "source": [
        "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k1yKOKbVjy2B"
      },
      "outputs": [],
      "source": [
        "# Instantiate memory\n",
        "memory = Memory(max_size = memory_size)\n",
        "\n",
        "# Render the environment\n",
        "game.new_episode()\n",
        "\n",
        "for i in range(pretrain_length):\n",
        "    # If it's the first step\n",
        "    if i == 0:\n",
        "        # First we need a state\n",
        "        state = game.get_state().screen_buffer\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    # Random action\n",
        "    action = random.choice(possible_actions)\n",
        "\n",
        "    # Get the rewards\n",
        "    reward = game.make_action(action)\n",
        "\n",
        "    # Look if the episode is finished\n",
        "    done = game.is_episode_finished()\n",
        "\n",
        "    # If we're dead\n",
        "    if done:\n",
        "        # We finished the episode\n",
        "        next_state = np.zeros(state.shape)\n",
        "\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "        # Start a new episode\n",
        "        game.new_episode()\n",
        "\n",
        "        # First we need a state\n",
        "        state = game.get_state().screen_buffer\n",
        "\n",
        "        # Stack the frames\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "    else:\n",
        "        # Get the next state\n",
        "        next_state = game.get_state().screen_buffer\n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "        # Our state is now the next_state\n",
        "        state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxosCT1ajy2C"
      },
      "source": [
        "## Step 7: Set up Tensorboard üìä\n",
        "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
        "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dxv-FiqWjy2C"
      },
      "outputs": [],
      "source": [
        "# Setup TensorBoard Writer\n",
        "writer = tf.compat.v1.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "## Losses\n",
        "tf.compat.v1.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "write_op = tf.compat.v1.summary.merge_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXR8b9TEjy2C"
      },
      "source": [
        "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
        "\n",
        "Our algorithm:\n",
        "<br>\n",
        "* Initialize the weights\n",
        "* Init the environment\n",
        "* Initialize the decay rate (that will use to reduce epsilon)\n",
        "<br><br>\n",
        "* **For** episode to max_episode **do**\n",
        "    * Make new episode\n",
        "    * Set step to 0\n",
        "    * Observe the first state $s_0$\n",
        "    <br><br>\n",
        "    * **While** step < max_steps **do**:\n",
        "        * Increase decay_rate\n",
        "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
        "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
        "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
        "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
        "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
        "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
        "    * **endfor**\n",
        "    <br><br>\n",
        "* **endfor**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UCO_qh3Qjy2D"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function will do the part\n",
        "With √è¬µ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
        "\"\"\"\n",
        "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
        "    ## EPSILON GREEDY STRATEGY\n",
        "    # Choose action a from state s using epsilon greedy.\n",
        "    ## First we randomize a number\n",
        "    exp_exp_tradeoff = np.random.rand()\n",
        "\n",
        "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
        "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "\n",
        "    if (explore_probability > exp_exp_tradeoff):\n",
        "        # Make a random action (exploration)\n",
        "        action = random.choice(possible_actions)\n",
        "\n",
        "    else:\n",
        "        # Get action from Q-network (exploitation)\n",
        "        # Estimate the Qs values state\n",
        "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "\n",
        "        # Take the biggest Q value (= the best action)\n",
        "        choice = np.argmax(Qs)\n",
        "        action = possible_actions[int(choice)]\n",
        "\n",
        "    return action, explore_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxTtUgyJjy2D",
        "outputId": "3589a04d-547c-4abe-8e4e-46c85471786f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved\n",
            "Episode: 1 Total reward: 70.0 Training loss: 55.7714 Explore P: 0.9925\n",
            "Episode: 4 Total reward: 95.0 Training loss: 71.7504 Explore P: 0.9821\n",
            "Model Saved\n",
            "Episode: 7 Total reward: 93.0 Training loss: 60.1338 Explore P: 0.9717\n",
            "Model Saved\n",
            "Episode: 13 Total reward: 69.0 Training loss: 278.8697 Explore P: 0.9454\n",
            "Model Saved\n",
            "Episode: 17 Total reward: 74.0 Training loss: 3048.4014 Explore P: 0.9295\n",
            "Episode: 18 Total reward: 94.0 Training loss: 85.6351 Explore P: 0.9288\n",
            "Model Saved\n",
            "Episode: 22 Total reward: 93.0 Training loss: 34.0719 Explore P: 0.9144\n",
            "Episode: 23 Total reward: 95.0 Training loss: 323.8544 Explore P: 0.9139\n",
            "Episode: 25 Total reward: 95.0 Training loss: 65.1755 Explore P: 0.9088\n",
            "Model Saved\n",
            "Episode: 29 Total reward: 41.0 Training loss: 505.8490 Explore P: 0.8910\n",
            "Episode: 30 Total reward: 94.0 Training loss: 4993.2573 Explore P: 0.8904\n",
            "Model Saved\n",
            "Episode: 33 Total reward: 95.0 Training loss: 22.9166 Explore P: 0.8811\n",
            "Model Saved\n",
            "Episode: 36 Total reward: 89.0 Training loss: 13.7117 Explore P: 0.8714\n",
            "Episode: 37 Total reward: 92.0 Training loss: 223.2921 Explore P: 0.8707\n",
            "Episode: 40 Total reward: 94.0 Training loss: 191.9988 Explore P: 0.8615\n",
            "Model Saved\n",
            "Model Saved\n",
            "Episode: 49 Total reward: 95.0 Training loss: 13.5757 Explore P: 0.8276\n",
            "Model Saved\n",
            "Episode: 54 Total reward: 45.0 Training loss: 730.1566 Explore P: 0.8078\n",
            "Episode: 55 Total reward: 95.0 Training loss: 144.8112 Explore P: 0.8073\n",
            "Model Saved\n",
            "Model Saved\n",
            "Episode: 63 Total reward: 93.0 Training loss: 66.0356 Explore P: 0.7792\n",
            "Episode: 64 Total reward: 89.0 Training loss: 39.7586 Explore P: 0.7783\n",
            "Model Saved\n",
            "Model Saved\n",
            "Episode: 73 Total reward: 93.0 Training loss: 26.3156 Explore P: 0.7476\n",
            "Episode: 74 Total reward: 66.0 Training loss: 3.8485 Explore P: 0.7454\n",
            "Model Saved\n",
            "Episode: 76 Total reward: 95.0 Training loss: 63.1325 Explore P: 0.7413\n",
            "Episode: 78 Total reward: 93.0 Training loss: 4286.5938 Explore P: 0.7371\n",
            "Episode: 80 Total reward: 43.0 Training loss: 6.9555 Explore P: 0.7300\n",
            "Model Saved\n",
            "Model Saved\n",
            "Episode: 87 Total reward: 74.0 Training loss: 6.9045 Explore P: 0.7072\n",
            "Episode: 88 Total reward: 92.0 Training loss: 14.6650 Explore P: 0.7065\n",
            "Model Saved\n",
            "Episode: 94 Total reward: 49.0 Training loss: 556.1763 Explore P: 0.6865\n",
            "Model Saved\n",
            "Episode: 97 Total reward: 94.0 Training loss: 11.6198 Explore P: 0.6793\n",
            "Episode: 98 Total reward: 49.0 Training loss: 50.2265 Explore P: 0.6765\n",
            "Episode: 100 Total reward: 63.0 Training loss: 170.0156 Explore P: 0.6710\n",
            "Model Saved\n",
            "Episode: 101 Total reward: 69.0 Training loss: 8.2997 Explore P: 0.6692\n",
            "Episode: 104 Total reward: 94.0 Training loss: 13.8320 Explore P: 0.6622\n",
            "Model Saved\n",
            "Episode: 108 Total reward: 85.0 Training loss: 10.0320 Explore P: 0.6514\n",
            "Episode: 110 Total reward: 73.0 Training loss: 3.7276 Explore P: 0.6468\n",
            "Model Saved\n",
            "Episode: 112 Total reward: 95.0 Training loss: 17.9492 Explore P: 0.6432\n",
            "Episode: 113 Total reward: 95.0 Training loss: 45.4802 Explore P: 0.6428\n",
            "Episode: 115 Total reward: 49.0 Training loss: 33.8531 Explore P: 0.6370\n",
            "Model Saved\n",
            "Episode: 119 Total reward: 94.0 Training loss: 7.0371 Explore P: 0.6273\n",
            "Model Saved\n",
            "Episode: 122 Total reward: 71.0 Training loss: 9.7134 Explore P: 0.6196\n",
            "Model Saved\n",
            "Episode: 126 Total reward: 65.0 Training loss: 10.4450 Explore P: 0.6087\n",
            "Episode: 127 Total reward: 68.0 Training loss: 6.6849 Explore P: 0.6070\n",
            "Episode: 130 Total reward: 41.0 Training loss: 10.8369 Explore P: 0.5981\n",
            "Model Saved\n",
            "Episode: 131 Total reward: 70.0 Training loss: 151.2753 Explore P: 0.5966\n",
            "Episode: 132 Total reward: 90.0 Training loss: 21.3814 Explore P: 0.5959\n",
            "Episode: 133 Total reward: 50.0 Training loss: 5.8134 Explore P: 0.5935\n",
            "Episode: 135 Total reward: 55.0 Training loss: 25.1881 Explore P: 0.5882\n",
            "Model Saved\n",
            "Episode: 136 Total reward: 64.0 Training loss: 44.6712 Explore P: 0.5864\n",
            "Episode: 138 Total reward: 59.0 Training loss: 10.3477 Explore P: 0.5814\n",
            "Episode: 139 Total reward: 75.0 Training loss: 38.5061 Explore P: 0.5802\n",
            "Model Saved\n",
            "Episode: 143 Total reward: 62.0 Training loss: 77.8459 Explore P: 0.5698\n",
            "Episode: 144 Total reward: 75.0 Training loss: 5.7877 Explore P: 0.5686\n",
            "Episode: 145 Total reward: 67.0 Training loss: 8.7916 Explore P: 0.5670\n",
            "Model Saved\n",
            "Episode: 147 Total reward: 68.0 Training loss: 19.1448 Explore P: 0.5627\n",
            "Episode: 148 Total reward: 95.0 Training loss: 12.2605 Explore P: 0.5624\n",
            "Episode: 149 Total reward: 93.0 Training loss: 44.7576 Explore P: 0.5619\n",
            "Model Saved\n",
            "Episode: 151 Total reward: 75.0 Training loss: 14.8601 Explore P: 0.5580\n",
            "Episode: 152 Total reward: 95.0 Training loss: 11.8209 Explore P: 0.5577\n",
            "Episode: 154 Total reward: 43.0 Training loss: 8.5263 Explore P: 0.5523\n",
            "Model Saved\n",
            "Episode: 157 Total reward: 95.0 Training loss: 27486.0762 Explore P: 0.5466\n",
            "Episode: 159 Total reward: 93.0 Training loss: 22.9077 Explore P: 0.5435\n",
            "Episode: 160 Total reward: 83.0 Training loss: 25.0737 Explore P: 0.5426\n",
            "Model Saved\n",
            "Episode: 161 Total reward: 51.0 Training loss: 20.8140 Explore P: 0.5404\n",
            "Episode: 162 Total reward: 86.0 Training loss: 13.7249 Explore P: 0.5396\n",
            "Episode: 164 Total reward: 45.0 Training loss: 18.1959 Explore P: 0.5346\n",
            "Episode: 165 Total reward: 48.0 Training loss: 24.5237 Explore P: 0.5323\n",
            "Model Saved\n",
            "Episode: 166 Total reward: 92.0 Training loss: 7.2944 Explore P: 0.5319\n",
            "Episode: 167 Total reward: 50.0 Training loss: 15.0623 Explore P: 0.5297\n",
            "Episode: 169 Total reward: 88.0 Training loss: 55.6715 Explore P: 0.5265\n",
            "Episode: 170 Total reward: 94.0 Training loss: 32.8356 Explore P: 0.5261\n",
            "Model Saved\n",
            "Episode: 171 Total reward: 51.0 Training loss: 14.7137 Explore P: 0.5240\n",
            "Episode: 174 Total reward: 90.0 Training loss: 11.2474 Explore P: 0.5184\n",
            "Episode: 175 Total reward: 48.0 Training loss: 12.2303 Explore P: 0.5162\n",
            "Model Saved\n",
            "Episode: 178 Total reward: 42.0 Training loss: 281.7925 Explore P: 0.5087\n",
            "Episode: 180 Total reward: 61.0 Training loss: 16.1188 Explore P: 0.5045\n",
            "Model Saved\n",
            "Episode: 181 Total reward: 93.0 Training loss: 10.1159 Explore P: 0.5041\n",
            "Episode: 182 Total reward: 62.0 Training loss: 86.5016 Explore P: 0.5024\n",
            "Model Saved\n",
            "Episode: 186 Total reward: 54.0 Training loss: 13.3997 Explore P: 0.4933\n",
            "Episode: 190 Total reward: 58.0 Training loss: 7.8483 Explore P: 0.4843\n",
            "Model Saved\n",
            "Episode: 191 Total reward: 42.0 Training loss: 30.8885 Explore P: 0.4820\n",
            "Episode: 192 Total reward: 48.0 Training loss: 10.8655 Explore P: 0.4799\n",
            "Episode: 193 Total reward: 78.0 Training loss: 16.4013 Explore P: 0.4789\n",
            "Episode: 195 Total reward: 89.0 Training loss: 19.3595 Explore P: 0.4760\n",
            "Model Saved\n",
            "Episode: 198 Total reward: 54.0 Training loss: 4.8752 Explore P: 0.4694\n",
            "Episode: 200 Total reward: 93.0 Training loss: 2.5489 Explore P: 0.4667\n",
            "Model Saved\n",
            "Episode: 203 Total reward: 95.0 Training loss: 13.1305 Explore P: 0.4619\n",
            "Episode: 205 Total reward: 47.0 Training loss: 8.6917 Explore P: 0.4577\n",
            "Model Saved\n",
            "Episode: 206 Total reward: 48.0 Training loss: 8.8139 Explore P: 0.4558\n",
            "Episode: 207 Total reward: 50.0 Training loss: 16.9426 Explore P: 0.4539\n",
            "Model Saved\n",
            "Episode: 212 Total reward: 64.0 Training loss: 4.9631 Explore P: 0.4438\n",
            "Episode: 213 Total reward: 45.0 Training loss: 16.2793 Explore P: 0.4418\n",
            "Episode: 214 Total reward: 86.0 Training loss: 29.0634 Explore P: 0.4411\n",
            "Model Saved\n",
            "Episode: 216 Total reward: 92.0 Training loss: 13.4946 Explore P: 0.4386\n",
            "Episode: 217 Total reward: 69.0 Training loss: 27.1137 Explore P: 0.4374\n",
            "Episode: 219 Total reward: 83.0 Training loss: 49.6235 Explore P: 0.4345\n",
            "Episode: 220 Total reward: 47.0 Training loss: 10.1510 Explore P: 0.4327\n",
            "Model Saved\n",
            "Episode: 222 Total reward: 67.0 Training loss: 20.0675 Explore P: 0.4293\n",
            "Episode: 223 Total reward: 88.0 Training loss: 49.9736 Explore P: 0.4288\n",
            "Episode: 225 Total reward: 86.0 Training loss: 36.7866 Explore P: 0.4261\n",
            "Model Saved\n",
            "Episode: 226 Total reward: 55.0 Training loss: 22.6398 Explore P: 0.4244\n",
            "Episode: 227 Total reward: 77.0 Training loss: 12.6794 Explore P: 0.4234\n",
            "Episode: 228 Total reward: 45.0 Training loss: 63.4399 Explore P: 0.4215\n",
            "Episode: 229 Total reward: 62.0 Training loss: 37.0764 Explore P: 0.4201\n",
            "Episode: 230 Total reward: 72.0 Training loss: 12.2503 Explore P: 0.4191\n",
            "Model Saved\n",
            "Episode: 231 Total reward: 46.0 Training loss: 26.2811 Explore P: 0.4173\n",
            "Episode: 232 Total reward: 85.0 Training loss: 23.4477 Explore P: 0.4166\n",
            "Episode: 233 Total reward: 69.0 Training loss: 11.1100 Explore P: 0.4155\n",
            "Episode: 234 Total reward: 43.0 Training loss: 12.9265 Explore P: 0.4136\n",
            "Model Saved\n",
            "Episode: 238 Total reward: 91.0 Training loss: 66.7607 Explore P: 0.4072\n",
            "Episode: 240 Total reward: 61.0 Training loss: 14.2862 Explore P: 0.4038\n",
            "Model Saved\n",
            "Episode: 242 Total reward: 65.0 Training loss: 12.1545 Explore P: 0.4005\n",
            "Episode: 245 Total reward: 89.0 Training loss: 15.4964 Explore P: 0.3961\n",
            "Model Saved\n",
            "Episode: 246 Total reward: 53.0 Training loss: 18.1497 Explore P: 0.3944\n",
            "Episode: 247 Total reward: 66.0 Training loss: 196.3454 Explore P: 0.3933\n",
            "Episode: 248 Total reward: 68.0 Training loss: 20.9618 Explore P: 0.3922\n",
            "Episode: 249 Total reward: 74.0 Training loss: 5.9601 Explore P: 0.3914\n",
            "Episode: 250 Total reward: 62.0 Training loss: 11.3909 Explore P: 0.3901\n",
            "Model Saved\n",
            "Episode: 252 Total reward: 70.0 Training loss: 7.7569 Explore P: 0.3872\n",
            "Episode: 253 Total reward: 80.0 Training loss: 54.4420 Explore P: 0.3864\n",
            "Episode: 255 Total reward: 94.0 Training loss: 4.7366 Explore P: 0.3843\n",
            "Model Saved\n",
            "Episode: 256 Total reward: 89.0 Training loss: 11.0257 Explore P: 0.3838\n",
            "Episode: 257 Total reward: 94.0 Training loss: 70.8463 Explore P: 0.3836\n",
            "Episode: 259 Total reward: 69.0 Training loss: 10.0234 Explore P: 0.3807\n",
            "Episode: 260 Total reward: 75.0 Training loss: 11.7616 Explore P: 0.3799\n",
            "Model Saved\n",
            "Episode: 262 Total reward: 93.0 Training loss: 4.8351 Explore P: 0.3778\n",
            "Episode: 264 Total reward: 48.0 Training loss: 16.1067 Explore P: 0.3744\n",
            "Episode: 265 Total reward: 69.0 Training loss: 15.5593 Explore P: 0.3734\n",
            "Model Saved\n",
            "Episode: 267 Total reward: 93.0 Training loss: 64.5029 Explore P: 0.3713\n",
            "Episode: 268 Total reward: 61.0 Training loss: 10.5372 Explore P: 0.3700\n",
            "Episode: 269 Total reward: 72.0 Training loss: 14.9904 Explore P: 0.3692\n",
            "Model Saved\n",
            "Episode: 271 Total reward: 93.0 Training loss: 9.1451 Explore P: 0.3671\n",
            "Episode: 273 Total reward: 95.0 Training loss: 9.5743 Explore P: 0.3651\n",
            "Episode: 275 Total reward: 84.0 Training loss: 82.5763 Explore P: 0.3627\n",
            "Model Saved\n",
            "Episode: 276 Total reward: 86.0 Training loss: 16.5064 Explore P: 0.3622\n",
            "Episode: 277 Total reward: 75.0 Training loss: 5.0453 Explore P: 0.3615\n",
            "Episode: 278 Total reward: 71.0 Training loss: 6.9814 Explore P: 0.3606\n",
            "Episode: 279 Total reward: 44.0 Training loss: 35.0190 Explore P: 0.3589\n",
            "Model Saved\n",
            "Episode: 282 Total reward: 92.0 Training loss: 7.4766 Explore P: 0.3552\n",
            "Episode: 283 Total reward: 65.0 Training loss: 30.1675 Explore P: 0.3541\n",
            "Episode: 284 Total reward: 44.0 Training loss: 5.0964 Explore P: 0.3525\n",
            "Episode: 285 Total reward: 86.0 Training loss: 20.3351 Explore P: 0.3520\n",
            "Model Saved\n",
            "Episode: 287 Total reward: 48.0 Training loss: 23.3126 Explore P: 0.3486\n",
            "Episode: 288 Total reward: 61.0 Training loss: 11.3569 Explore P: 0.3474\n",
            "Episode: 289 Total reward: 93.0 Training loss: 62.2500 Explore P: 0.3472\n",
            "Episode: 290 Total reward: 45.0 Training loss: 18.6558 Explore P: 0.3456\n",
            "Model Saved\n",
            "Episode: 291 Total reward: 49.0 Training loss: 40.7223 Explore P: 0.3442\n",
            "Episode: 292 Total reward: 80.0 Training loss: 7.7551 Explore P: 0.3435\n",
            "Episode: 293 Total reward: 74.0 Training loss: 51.0232 Explore P: 0.3426\n",
            "Episode: 294 Total reward: 44.0 Training loss: 8.6977 Explore P: 0.3411\n",
            "Model Saved\n",
            "Episode: 296 Total reward: 71.0 Training loss: 20.5033 Explore P: 0.3386\n",
            "Episode: 298 Total reward: 82.0 Training loss: 5.7185 Explore P: 0.3363\n",
            "Episode: 300 Total reward: 52.0 Training loss: 6.0367 Explore P: 0.3334\n",
            "Model Saved\n",
            "Episode: 301 Total reward: 42.0 Training loss: 8.0007 Explore P: 0.3319\n",
            "Episode: 303 Total reward: 87.0 Training loss: 23.2884 Explore P: 0.3298\n",
            "Episode: 304 Total reward: 92.0 Training loss: 9.2451 Explore P: 0.3295\n",
            "Episode: 305 Total reward: 55.0 Training loss: 6.2748 Explore P: 0.3282\n",
            "Model Saved\n",
            "Episode: 306 Total reward: 70.0 Training loss: 14.0112 Explore P: 0.3274\n",
            "Episode: 308 Total reward: 62.0 Training loss: 29.6523 Explore P: 0.3247\n",
            "Episode: 309 Total reward: 66.0 Training loss: 8.3417 Explore P: 0.3238\n",
            "Episode: 310 Total reward: 95.0 Training loss: 21.6139 Explore P: 0.3236\n",
            "Model Saved\n",
            "Episode: 312 Total reward: 92.0 Training loss: 10.9744 Explore P: 0.3218\n",
            "Episode: 313 Total reward: 76.0 Training loss: 18.5931 Explore P: 0.3211\n",
            "Episode: 315 Total reward: 50.0 Training loss: 6.3037 Explore P: 0.3182\n",
            "Model Saved\n",
            "Episode: 317 Total reward: 61.0 Training loss: 9.6760 Explore P: 0.3155\n",
            "Episode: 318 Total reward: 61.0 Training loss: 13.9380 Explore P: 0.3145\n",
            "Episode: 320 Total reward: 44.0 Training loss: 9.1424 Explore P: 0.3115\n",
            "Model Saved\n",
            "Episode: 321 Total reward: 48.0 Training loss: 13.7048 Explore P: 0.3102\n",
            "Episode: 322 Total reward: 91.0 Training loss: 10.4104 Explore P: 0.3099\n",
            "Episode: 323 Total reward: 60.0 Training loss: 36.7820 Explore P: 0.3089\n",
            "Episode: 324 Total reward: 87.0 Training loss: 28.1534 Explore P: 0.3085\n",
            "Episode: 325 Total reward: 76.0 Training loss: 193.2973 Explore P: 0.3077\n",
            "Model Saved\n",
            "Episode: 326 Total reward: 91.0 Training loss: 6.6115 Explore P: 0.3074\n",
            "Episode: 328 Total reward: 91.0 Training loss: 9.8757 Explore P: 0.3056\n",
            "Episode: 329 Total reward: 53.0 Training loss: 9.7289 Explore P: 0.3045\n",
            "Episode: 330 Total reward: 57.0 Training loss: 7.6436 Explore P: 0.3034\n",
            "Model Saved\n",
            "Episode: 331 Total reward: 41.0 Training loss: 9.0067 Explore P: 0.3019\n",
            "Episode: 332 Total reward: 75.0 Training loss: 29.9809 Explore P: 0.3011\n",
            "Episode: 333 Total reward: 62.0 Training loss: 20.5272 Explore P: 0.3002\n",
            "Episode: 334 Total reward: 64.0 Training loss: 10.0951 Explore P: 0.2992\n",
            "Episode: 335 Total reward: 57.0 Training loss: 41.8582 Explore P: 0.2982\n",
            "Model Saved\n",
            "Episode: 336 Total reward: 95.0 Training loss: 17.2259 Explore P: 0.2981\n",
            "Episode: 337 Total reward: 91.0 Training loss: 23.5857 Explore P: 0.2978\n",
            "Episode: 340 Total reward: 73.0 Training loss: 11.5059 Explore P: 0.2943\n",
            "Model Saved\n",
            "Episode: 341 Total reward: 94.0 Training loss: 10.1146 Explore P: 0.2941\n",
            "Episode: 343 Total reward: 93.0 Training loss: 9.9269 Explore P: 0.2924\n",
            "Episode: 344 Total reward: 90.0 Training loss: 6.3672 Explore P: 0.2921\n",
            "Episode: 345 Total reward: 72.0 Training loss: 8.3581 Explore P: 0.2914\n",
            "Model Saved\n",
            "Episode: 346 Total reward: 42.0 Training loss: 16.9680 Explore P: 0.2901\n",
            "Episode: 347 Total reward: 59.0 Training loss: 73.0180 Explore P: 0.2890\n",
            "Episode: 348 Total reward: 60.0 Training loss: 10.6417 Explore P: 0.2880\n",
            "Episode: 349 Total reward: 62.0 Training loss: 13.7402 Explore P: 0.2871\n",
            "Model Saved\n",
            "Episode: 351 Total reward: 62.0 Training loss: 10.1744 Explore P: 0.2848\n",
            "Episode: 352 Total reward: 56.0 Training loss: 14.3365 Explore P: 0.2837\n",
            "Episode: 353 Total reward: 95.0 Training loss: 13.5013 Explore P: 0.2835\n",
            "Episode: 354 Total reward: 59.0 Training loss: 12.7164 Explore P: 0.2825\n",
            "Episode: 355 Total reward: 94.0 Training loss: 7.5589 Explore P: 0.2823\n",
            "Model Saved\n",
            "Episode: 356 Total reward: 44.0 Training loss: 17.4465 Explore P: 0.2810\n",
            "Episode: 357 Total reward: 62.0 Training loss: 10.1810 Explore P: 0.2801\n",
            "Episode: 358 Total reward: 62.0 Training loss: 9.9267 Explore P: 0.2792\n",
            "Episode: 359 Total reward: 53.0 Training loss: 12.8236 Explore P: 0.2782\n",
            "Model Saved\n",
            "Episode: 361 Total reward: 75.0 Training loss: 196.6754 Explore P: 0.2763\n",
            "Episode: 362 Total reward: 47.0 Training loss: 16.5597 Explore P: 0.2751\n",
            "Episode: 364 Total reward: 41.0 Training loss: 11.7535 Explore P: 0.2725\n",
            "Episode: 365 Total reward: 44.0 Training loss: 6.6872 Explore P: 0.2712\n",
            "Model Saved\n",
            "Episode: 366 Total reward: 90.0 Training loss: 50.2224 Explore P: 0.2709\n",
            "Episode: 367 Total reward: 91.0 Training loss: 9.6391 Explore P: 0.2707\n",
            "Episode: 368 Total reward: 95.0 Training loss: 19.3375 Explore P: 0.2705\n",
            "Episode: 369 Total reward: 70.0 Training loss: 8.0501 Explore P: 0.2697\n",
            "Episode: 370 Total reward: 76.0 Training loss: 10.3757 Explore P: 0.2692\n",
            "Model Saved\n",
            "Episode: 371 Total reward: 71.0 Training loss: 21.1313 Explore P: 0.2686\n",
            "Episode: 372 Total reward: 64.0 Training loss: 17.6892 Explore P: 0.2677\n",
            "Episode: 373 Total reward: 57.0 Training loss: 13.9812 Explore P: 0.2669\n",
            "Episode: 374 Total reward: 59.0 Training loss: 17.4681 Explore P: 0.2659\n",
            "Model Saved\n",
            "Episode: 376 Total reward: 81.0 Training loss: 32.3152 Explore P: 0.2641\n",
            "Episode: 377 Total reward: 71.0 Training loss: 7.3545 Explore P: 0.2635\n",
            "Episode: 378 Total reward: 70.0 Training loss: 20.9850 Explore P: 0.2628\n",
            "Episode: 379 Total reward: 59.0 Training loss: 63.2264 Explore P: 0.2619\n",
            "Episode: 380 Total reward: 48.0 Training loss: 28.4776 Explore P: 0.2608\n",
            "Model Saved\n",
            "Episode: 382 Total reward: 95.0 Training loss: 8.3479 Explore P: 0.2594\n",
            "Episode: 383 Total reward: 45.0 Training loss: 11.8927 Explore P: 0.2583\n",
            "Episode: 384 Total reward: 51.0 Training loss: 12.5012 Explore P: 0.2573\n",
            "Episode: 385 Total reward: 62.0 Training loss: 9.0212 Explore P: 0.2564\n",
            "Model Saved\n",
            "Episode: 386 Total reward: 82.0 Training loss: 13.3636 Explore P: 0.2560\n",
            "Episode: 387 Total reward: 73.0 Training loss: 20.3901 Explore P: 0.2554\n",
            "Episode: 388 Total reward: 70.0 Training loss: 15.9902 Explore P: 0.2548\n",
            "Episode: 389 Total reward: 63.0 Training loss: 11.2696 Explore P: 0.2540\n",
            "Episode: 390 Total reward: 81.0 Training loss: 11.1806 Explore P: 0.2535\n",
            "Model Saved\n",
            "Episode: 391 Total reward: 74.0 Training loss: 13.9891 Explore P: 0.2528\n",
            "Episode: 392 Total reward: 66.0 Training loss: 62.4742 Explore P: 0.2521\n",
            "Episode: 393 Total reward: 95.0 Training loss: 11.4260 Explore P: 0.2519\n",
            "Episode: 394 Total reward: 92.0 Training loss: 10.8748 Explore P: 0.2517\n",
            "Episode: 395 Total reward: 62.0 Training loss: 9.3395 Explore P: 0.2509\n",
            "Model Saved\n",
            "Episode: 397 Total reward: 87.0 Training loss: 69.0663 Explore P: 0.2494\n",
            "Episode: 398 Total reward: 78.0 Training loss: 11.1465 Explore P: 0.2488\n",
            "Episode: 399 Total reward: 73.0 Training loss: 30.7507 Explore P: 0.2482\n",
            "Episode: 400 Total reward: 94.0 Training loss: 4.1846 Explore P: 0.2480\n",
            "Model Saved\n",
            "Episode: 401 Total reward: 53.0 Training loss: 22.8764 Explore P: 0.2470\n",
            "Episode: 402 Total reward: 59.0 Training loss: 15.6096 Explore P: 0.2461\n",
            "Episode: 403 Total reward: 72.0 Training loss: 9.0625 Explore P: 0.2455\n",
            "Episode: 404 Total reward: 72.0 Training loss: 15.8503 Explore P: 0.2448\n",
            "Episode: 405 Total reward: 87.0 Training loss: 8.9901 Explore P: 0.2445\n",
            "Model Saved\n",
            "Episode: 406 Total reward: 74.0 Training loss: 9.4076 Explore P: 0.2440\n",
            "Episode: 407 Total reward: 87.0 Training loss: 100.8266 Explore P: 0.2437\n",
            "Episode: 409 Total reward: 68.0 Training loss: 12.1154 Explore P: 0.2419\n",
            "Episode: 410 Total reward: 95.0 Training loss: 26.2855 Explore P: 0.2417\n",
            "Model Saved\n",
            "Episode: 411 Total reward: 50.0 Training loss: 15.3486 Explore P: 0.2408\n",
            "Episode: 413 Total reward: 58.0 Training loss: 6.3626 Explore P: 0.2387\n",
            "Episode: 414 Total reward: 65.0 Training loss: 47.4161 Explore P: 0.2380\n",
            "Model Saved\n",
            "Episode: 416 Total reward: 57.0 Training loss: 18.0335 Explore P: 0.2361\n",
            "Episode: 417 Total reward: 95.0 Training loss: 15.1316 Explore P: 0.2360\n",
            "Episode: 418 Total reward: 75.0 Training loss: 10.4553 Explore P: 0.2355\n",
            "Episode: 419 Total reward: 74.0 Training loss: 13.2152 Explore P: 0.2350\n",
            "Episode: 420 Total reward: 46.0 Training loss: 12.8342 Explore P: 0.2340\n",
            "Model Saved\n",
            "Episode: 421 Total reward: 93.0 Training loss: 5.3866 Explore P: 0.2338\n",
            "Episode: 422 Total reward: 95.0 Training loss: 11.4808 Explore P: 0.2337\n",
            "Episode: 423 Total reward: 70.0 Training loss: 10.6552 Explore P: 0.2331\n",
            "Episode: 424 Total reward: 95.0 Training loss: 10.4848 Explore P: 0.2330\n",
            "Episode: 425 Total reward: 94.0 Training loss: 10.4051 Explore P: 0.2328\n",
            "Model Saved\n",
            "Episode: 426 Total reward: 88.0 Training loss: 9.4462 Explore P: 0.2325\n",
            "Episode: 427 Total reward: 89.0 Training loss: 42.8050 Explore P: 0.2323\n",
            "Episode: 429 Total reward: 47.0 Training loss: 19.6124 Explore P: 0.2302\n",
            "Episode: 430 Total reward: 94.0 Training loss: 26.3922 Explore P: 0.2300\n",
            "Model Saved\n",
            "Episode: 431 Total reward: 69.0 Training loss: 9.1853 Explore P: 0.2294\n",
            "Episode: 432 Total reward: 83.0 Training loss: 15.4866 Explore P: 0.2291\n",
            "Episode: 433 Total reward: 48.0 Training loss: 5.1251 Explore P: 0.2281\n",
            "Episode: 434 Total reward: 63.0 Training loss: 10.1555 Explore P: 0.2274\n",
            "Model Saved\n",
            "Episode: 436 Total reward: 67.0 Training loss: 6.5563 Explore P: 0.2257\n",
            "Episode: 437 Total reward: 84.0 Training loss: 26.1320 Explore P: 0.2253\n",
            "Episode: 438 Total reward: 81.0 Training loss: 10.5546 Explore P: 0.2249\n",
            "Episode: 439 Total reward: 95.0 Training loss: 4.5009 Explore P: 0.2248\n",
            "Episode: 440 Total reward: 66.0 Training loss: 10.5066 Explore P: 0.2241\n",
            "Model Saved\n",
            "Episode: 441 Total reward: 74.0 Training loss: 15.2149 Explore P: 0.2235\n",
            "Episode: 443 Total reward: 61.0 Training loss: 22.5830 Explore P: 0.2217\n",
            "Episode: 444 Total reward: 60.0 Training loss: 109.8802 Explore P: 0.2210\n",
            "Episode: 445 Total reward: 65.0 Training loss: 12.6639 Explore P: 0.2203\n",
            "Model Saved\n",
            "Episode: 446 Total reward: 95.0 Training loss: 12.3011 Explore P: 0.2202\n",
            "Episode: 447 Total reward: 92.0 Training loss: 13.2559 Explore P: 0.2200\n",
            "Episode: 448 Total reward: 72.0 Training loss: 37.1317 Explore P: 0.2195\n",
            "Episode: 449 Total reward: 50.0 Training loss: 13.8196 Explore P: 0.2186\n",
            "Episode: 450 Total reward: 91.0 Training loss: 12.0581 Explore P: 0.2184\n",
            "Model Saved\n",
            "Episode: 451 Total reward: 95.0 Training loss: 5.8961 Explore P: 0.2183\n",
            "Episode: 452 Total reward: 75.0 Training loss: 11.3298 Explore P: 0.2179\n",
            "Episode: 454 Total reward: 90.0 Training loss: 20.8315 Explore P: 0.2166\n",
            "Episode: 455 Total reward: 77.0 Training loss: 26.2795 Explore P: 0.2161\n",
            "Model Saved\n",
            "Episode: 456 Total reward: 45.0 Training loss: 15.8848 Explore P: 0.2152\n",
            "Episode: 457 Total reward: 59.0 Training loss: 19.9564 Explore P: 0.2144\n",
            "Episode: 458 Total reward: 48.0 Training loss: 13.8690 Explore P: 0.2135\n",
            "Episode: 459 Total reward: 50.0 Training loss: 21.9060 Explore P: 0.2127\n",
            "Model Saved\n",
            "Episode: 461 Total reward: 64.0 Training loss: 7.7435 Explore P: 0.2110\n",
            "Episode: 463 Total reward: 79.0 Training loss: 9.3721 Explore P: 0.2096\n",
            "Episode: 464 Total reward: 90.0 Training loss: 8.4734 Explore P: 0.2094\n",
            "Episode: 465 Total reward: 69.0 Training loss: 15.3102 Explore P: 0.2088\n",
            "Model Saved\n",
            "Episode: 466 Total reward: 95.0 Training loss: 12.1698 Explore P: 0.2087\n",
            "Episode: 467 Total reward: 70.0 Training loss: 7.3788 Explore P: 0.2082\n",
            "Episode: 468 Total reward: 72.0 Training loss: 7.6849 Explore P: 0.2077\n",
            "Episode: 469 Total reward: 52.0 Training loss: 11.5004 Explore P: 0.2069\n",
            "Model Saved\n",
            "Episode: 471 Total reward: 51.0 Training loss: 9.3903 Explore P: 0.2051\n",
            "Episode: 472 Total reward: 64.0 Training loss: 21.9127 Explore P: 0.2045\n",
            "Episode: 473 Total reward: 95.0 Training loss: 8.6246 Explore P: 0.2044\n",
            "Episode: 475 Total reward: 63.0 Training loss: 25.9687 Explore P: 0.2028\n",
            "Model Saved\n",
            "Episode: 476 Total reward: 81.0 Training loss: 17.2828 Explore P: 0.2024\n",
            "Episode: 477 Total reward: 95.0 Training loss: 4.2493 Explore P: 0.2023\n",
            "Episode: 478 Total reward: 54.0 Training loss: 17.2075 Explore P: 0.2014\n",
            "Episode: 479 Total reward: 88.0 Training loss: 11.5745 Explore P: 0.2012\n",
            "Episode: 480 Total reward: 50.0 Training loss: 9.0978 Explore P: 0.2004\n",
            "Model Saved\n",
            "Episode: 481 Total reward: 63.0 Training loss: 21.3513 Explore P: 0.1998\n",
            "Episode: 482 Total reward: 52.0 Training loss: 22.7070 Explore P: 0.1990\n",
            "Episode: 483 Total reward: 93.0 Training loss: 7.9564 Explore P: 0.1989\n",
            "Episode: 484 Total reward: 56.0 Training loss: 4.6275 Explore P: 0.1982\n",
            "Episode: 485 Total reward: 70.0 Training loss: 62.5612 Explore P: 0.1977\n",
            "Model Saved\n",
            "Episode: 486 Total reward: 95.0 Training loss: 5.5562 Explore P: 0.1976\n",
            "Episode: 487 Total reward: 47.0 Training loss: 6.1026 Explore P: 0.1968\n",
            "Episode: 488 Total reward: 95.0 Training loss: 27.8405 Explore P: 0.1967\n",
            "Episode: 489 Total reward: 48.0 Training loss: 12.3871 Explore P: 0.1959\n",
            "Episode: 490 Total reward: 52.0 Training loss: 4.6968 Explore P: 0.1952\n",
            "Model Saved\n",
            "Episode: 491 Total reward: 41.0 Training loss: 28.5473 Explore P: 0.1943\n",
            "Episode: 492 Total reward: 94.0 Training loss: 19.0072 Explore P: 0.1941\n",
            "Episode: 493 Total reward: 92.0 Training loss: 6.8534 Explore P: 0.1940\n",
            "Episode: 495 Total reward: 89.0 Training loss: 15.7105 Explore P: 0.1928\n",
            "Model Saved\n",
            "Episode: 496 Total reward: 54.0 Training loss: 8.2926 Explore P: 0.1921\n",
            "Episode: 497 Total reward: 95.0 Training loss: 11.6984 Explore P: 0.1920\n",
            "Episode: 499 Total reward: 84.0 Training loss: 2.9242 Explore P: 0.1908\n",
            "Episode: 500 Total reward: 87.0 Training loss: 33.8103 Explore P: 0.1906\n",
            "Model Saved\n",
            "Episode: 502 Total reward: 58.0 Training loss: 14.9146 Explore P: 0.1890\n",
            "Episode: 503 Total reward: 71.0 Training loss: 13.9735 Explore P: 0.1885\n",
            "Episode: 504 Total reward: 62.0 Training loss: 8.2086 Explore P: 0.1879\n",
            "Episode: 505 Total reward: 75.0 Training loss: 26.0567 Explore P: 0.1875\n",
            "Model Saved\n",
            "Episode: 506 Total reward: 93.0 Training loss: 11.0387 Explore P: 0.1873\n",
            "Episode: 507 Total reward: 65.0 Training loss: 10.8137 Explore P: 0.1868\n",
            "Episode: 508 Total reward: 37.0 Training loss: 21.4364 Explore P: 0.1859\n",
            "Episode: 509 Total reward: 59.0 Training loss: 8.9677 Explore P: 0.1853\n",
            "Episode: 510 Total reward: 95.0 Training loss: 32.4707 Explore P: 0.1852\n",
            "Model Saved\n",
            "Episode: 511 Total reward: 59.0 Training loss: 44.1668 Explore P: 0.1845\n",
            "Episode: 512 Total reward: 85.0 Training loss: 8.3793 Explore P: 0.1842\n",
            "Episode: 513 Total reward: 95.0 Training loss: 11.5291 Explore P: 0.1841\n",
            "Episode: 514 Total reward: 58.0 Training loss: 15.1483 Explore P: 0.1834\n",
            "Episode: 515 Total reward: 85.0 Training loss: 10.0667 Explore P: 0.1831\n",
            "Model Saved\n",
            "Episode: 516 Total reward: 45.0 Training loss: 4.8517 Explore P: 0.1823\n",
            "Episode: 517 Total reward: 55.0 Training loss: 12.5245 Explore P: 0.1816\n",
            "Episode: 518 Total reward: 85.0 Training loss: 10.9803 Explore P: 0.1813\n",
            "Episode: 519 Total reward: 60.0 Training loss: 9.8959 Explore P: 0.1807\n",
            "Episode: 520 Total reward: 80.0 Training loss: 6.9063 Explore P: 0.1804\n",
            "Model Saved\n",
            "Episode: 521 Total reward: 89.0 Training loss: 8.5222 Explore P: 0.1802\n",
            "Episode: 522 Total reward: 57.0 Training loss: 5.3933 Explore P: 0.1796\n",
            "Episode: 523 Total reward: 90.0 Training loss: 12.2235 Explore P: 0.1794\n",
            "Episode: 524 Total reward: 54.0 Training loss: 10.7480 Explore P: 0.1787\n",
            "Episode: 525 Total reward: 53.0 Training loss: 13.5621 Explore P: 0.1780\n",
            "Model Saved\n",
            "Episode: 526 Total reward: 95.0 Training loss: 18.8367 Explore P: 0.1779\n",
            "Episode: 527 Total reward: 71.0 Training loss: 19.6549 Explore P: 0.1774\n",
            "Episode: 528 Total reward: 89.0 Training loss: 16.2025 Explore P: 0.1772\n",
            "Episode: 529 Total reward: 90.0 Training loss: 12.2597 Explore P: 0.1771\n",
            "Episode: 530 Total reward: 50.0 Training loss: 20.2223 Explore P: 0.1763\n",
            "Model Saved\n",
            "Episode: 531 Total reward: 48.0 Training loss: 13.0183 Explore P: 0.1756\n",
            "Episode: 532 Total reward: 89.0 Training loss: 5.0873 Explore P: 0.1754\n",
            "Episode: 533 Total reward: 66.0 Training loss: 8.8935 Explore P: 0.1749\n",
            "Episode: 534 Total reward: 77.0 Training loss: 38.8264 Explore P: 0.1745\n",
            "Model Saved\n",
            "Episode: 536 Total reward: 91.0 Training loss: 11.7658 Explore P: 0.1735\n",
            "Episode: 537 Total reward: 58.0 Training loss: 10.0387 Explore P: 0.1729\n",
            "Episode: 538 Total reward: 60.0 Training loss: 13.1749 Explore P: 0.1723\n",
            "Episode: 539 Total reward: 68.0 Training loss: 17.8029 Explore P: 0.1718\n",
            "Episode: 540 Total reward: 57.0 Training loss: 16.5779 Explore P: 0.1712\n",
            "Model Saved\n",
            "Episode: 541 Total reward: 60.0 Training loss: 6.5179 Explore P: 0.1706\n",
            "Episode: 542 Total reward: 91.0 Training loss: 7.0390 Explore P: 0.1705\n",
            "Episode: 543 Total reward: 64.0 Training loss: 9.4757 Explore P: 0.1700\n",
            "Episode: 544 Total reward: 55.0 Training loss: 14.7871 Explore P: 0.1693\n",
            "Episode: 545 Total reward: 66.0 Training loss: 14.5383 Explore P: 0.1688\n",
            "Model Saved\n",
            "Episode: 546 Total reward: 72.0 Training loss: 17.2535 Explore P: 0.1684\n",
            "Episode: 547 Total reward: 82.0 Training loss: 5.3603 Explore P: 0.1681\n",
            "Episode: 548 Total reward: 71.0 Training loss: 9.5530 Explore P: 0.1677\n",
            "Episode: 549 Total reward: 92.0 Training loss: 34.8004 Explore P: 0.1676\n",
            "Episode: 550 Total reward: 64.0 Training loss: 13.2703 Explore P: 0.1671\n",
            "Model Saved\n",
            "Episode: 551 Total reward: 93.0 Training loss: 6.8461 Explore P: 0.1670\n",
            "Episode: 552 Total reward: 75.0 Training loss: 11.0168 Explore P: 0.1666\n",
            "Episode: 553 Total reward: 93.0 Training loss: 8.5234 Explore P: 0.1665\n",
            "Episode: 554 Total reward: 43.0 Training loss: 16.5473 Explore P: 0.1658\n",
            "Episode: 555 Total reward: 94.0 Training loss: 58.2916 Explore P: 0.1657\n",
            "Model Saved\n",
            "Episode: 556 Total reward: 63.0 Training loss: 6.4903 Explore P: 0.1652\n",
            "Episode: 557 Total reward: 66.0 Training loss: 8.8470 Explore P: 0.1647\n",
            "Episode: 558 Total reward: 64.0 Training loss: 5.6282 Explore P: 0.1642\n",
            "Episode: 559 Total reward: 84.0 Training loss: 12.7212 Explore P: 0.1639\n",
            "Episode: 560 Total reward: 74.0 Training loss: 29.2793 Explore P: 0.1636\n",
            "Model Saved\n",
            "Episode: 561 Total reward: 94.0 Training loss: 8.1002 Explore P: 0.1635\n",
            "Episode: 562 Total reward: 71.0 Training loss: 7.4615 Explore P: 0.1631\n",
            "Episode: 563 Total reward: 43.0 Training loss: 6.5603 Explore P: 0.1624\n",
            "Episode: 564 Total reward: 77.0 Training loss: 5.4042 Explore P: 0.1620\n",
            "Episode: 565 Total reward: 71.0 Training loss: 6.6104 Explore P: 0.1616\n",
            "Model Saved\n",
            "Episode: 566 Total reward: 61.0 Training loss: 5.3843 Explore P: 0.1611\n",
            "Episode: 567 Total reward: 95.0 Training loss: 17.4780 Explore P: 0.1610\n",
            "Episode: 568 Total reward: 48.0 Training loss: 6.3579 Explore P: 0.1604\n",
            "Episode: 569 Total reward: 73.0 Training loss: 12.3581 Explore P: 0.1599\n",
            "Episode: 570 Total reward: 75.0 Training loss: 14.4975 Explore P: 0.1596\n",
            "Model Saved\n",
            "Episode: 571 Total reward: 66.0 Training loss: 13.7879 Explore P: 0.1592\n",
            "Episode: 572 Total reward: 56.0 Training loss: 24.1584 Explore P: 0.1586\n",
            "Episode: 573 Total reward: 68.0 Training loss: 7.7456 Explore P: 0.1582\n",
            "Episode: 574 Total reward: 77.0 Training loss: 8.3256 Explore P: 0.1578\n",
            "Episode: 575 Total reward: 62.0 Training loss: 4.7121 Explore P: 0.1573\n",
            "Model Saved\n",
            "Episode: 576 Total reward: 41.0 Training loss: 16.6787 Explore P: 0.1566\n",
            "Episode: 577 Total reward: 69.0 Training loss: 5.3055 Explore P: 0.1561\n",
            "Episode: 578 Total reward: 93.0 Training loss: 20.8451 Explore P: 0.1560\n",
            "Episode: 579 Total reward: 62.0 Training loss: 4.8075 Explore P: 0.1555\n",
            "Episode: 580 Total reward: 70.0 Training loss: 13.5606 Explore P: 0.1551\n",
            "Model Saved\n",
            "Episode: 581 Total reward: 61.0 Training loss: 10.8229 Explore P: 0.1546\n",
            "Episode: 582 Total reward: 42.0 Training loss: 13.9453 Explore P: 0.1539\n",
            "Episode: 583 Total reward: 95.0 Training loss: 8.9567 Explore P: 0.1538\n",
            "Episode: 584 Total reward: 61.0 Training loss: 15.7424 Explore P: 0.1533\n",
            "Episode: 585 Total reward: 94.0 Training loss: 10.4743 Explore P: 0.1532\n",
            "Model Saved\n",
            "Episode: 586 Total reward: 85.0 Training loss: 8.7486 Explore P: 0.1530\n",
            "Episode: 587 Total reward: 76.0 Training loss: 50.0807 Explore P: 0.1527\n",
            "Episode: 588 Total reward: 71.0 Training loss: 16.0254 Explore P: 0.1523\n",
            "Episode: 589 Total reward: 50.0 Training loss: 26.0481 Explore P: 0.1517\n",
            "Episode: 590 Total reward: 70.0 Training loss: 10.0401 Explore P: 0.1513\n",
            "Model Saved\n",
            "Episode: 591 Total reward: 93.0 Training loss: 10.1555 Explore P: 0.1512\n",
            "Episode: 592 Total reward: 67.0 Training loss: 3.9725 Explore P: 0.1508\n",
            "Episode: 593 Total reward: 59.0 Training loss: 40.4599 Explore P: 0.1503\n",
            "Episode: 594 Total reward: 70.0 Training loss: 17.4026 Explore P: 0.1499\n",
            "Episode: 595 Total reward: 59.0 Training loss: 12.7433 Explore P: 0.1494\n",
            "Model Saved\n",
            "Episode: 596 Total reward: 48.0 Training loss: 15.6050 Explore P: 0.1488\n",
            "Episode: 597 Total reward: 64.0 Training loss: 10.4976 Explore P: 0.1484\n",
            "Episode: 598 Total reward: 81.0 Training loss: 6.6853 Explore P: 0.1481\n",
            "Episode: 599 Total reward: 95.0 Training loss: 21.6156 Explore P: 0.1480\n",
            "Model Saved\n",
            "Episode: 601 Total reward: 62.0 Training loss: 8.5776 Explore P: 0.1468\n",
            "Episode: 602 Total reward: 95.0 Training loss: 9.7405 Explore P: 0.1468\n",
            "Episode: 603 Total reward: 61.0 Training loss: 9.3136 Explore P: 0.1463\n",
            "Episode: 604 Total reward: 54.0 Training loss: 9.5690 Explore P: 0.1457\n",
            "Episode: 605 Total reward: 94.0 Training loss: 12.1273 Explore P: 0.1456\n",
            "Model Saved\n",
            "Episode: 606 Total reward: 63.0 Training loss: 14.8460 Explore P: 0.1452\n",
            "Episode: 607 Total reward: 55.0 Training loss: 10.0710 Explore P: 0.1446\n",
            "Episode: 608 Total reward: 64.0 Training loss: 11.1970 Explore P: 0.1442\n",
            "Episode: 609 Total reward: 82.0 Training loss: 9.0644 Explore P: 0.1439\n",
            "Episode: 610 Total reward: 90.0 Training loss: 29.9561 Explore P: 0.1438\n",
            "Model Saved\n",
            "Episode: 611 Total reward: 61.0 Training loss: 5.8331 Explore P: 0.1433\n",
            "Episode: 612 Total reward: 48.0 Training loss: 5.1610 Explore P: 0.1427\n",
            "Episode: 613 Total reward: 80.0 Training loss: 8.4959 Explore P: 0.1425\n",
            "Episode: 614 Total reward: 61.0 Training loss: 76.5893 Explore P: 0.1420\n",
            "Episode: 615 Total reward: 76.0 Training loss: 18.2958 Explore P: 0.1417\n",
            "Model Saved\n",
            "Episode: 616 Total reward: 66.0 Training loss: 11.4226 Explore P: 0.1413\n",
            "Episode: 617 Total reward: 57.0 Training loss: 5.8786 Explore P: 0.1408\n",
            "Episode: 618 Total reward: 74.0 Training loss: 20.4459 Explore P: 0.1405\n",
            "Episode: 619 Total reward: 94.0 Training loss: 9.1274 Explore P: 0.1404\n",
            "Episode: 620 Total reward: 63.0 Training loss: 23.6828 Explore P: 0.1399\n",
            "Model Saved\n",
            "Episode: 621 Total reward: 67.0 Training loss: 15.1809 Explore P: 0.1395\n",
            "Episode: 622 Total reward: 64.0 Training loss: 4.1257 Explore P: 0.1391\n",
            "Episode: 623 Total reward: 65.0 Training loss: 24.9372 Explore P: 0.1386\n",
            "Episode: 624 Total reward: 94.0 Training loss: 9.9296 Explore P: 0.1385\n",
            "Episode: 625 Total reward: 83.0 Training loss: 12.6238 Explore P: 0.1383\n",
            "Model Saved\n",
            "Episode: 626 Total reward: 95.0 Training loss: 8.3542 Explore P: 0.1382\n",
            "Episode: 627 Total reward: 95.0 Training loss: 29.9794 Explore P: 0.1382\n",
            "Episode: 628 Total reward: 87.0 Training loss: 7.9144 Explore P: 0.1380\n",
            "Episode: 630 Total reward: 86.0 Training loss: 9.5700 Explore P: 0.1372\n",
            "Model Saved\n",
            "Episode: 631 Total reward: 68.0 Training loss: 19.7794 Explore P: 0.1368\n",
            "Episode: 633 Total reward: 52.0 Training loss: 11.1502 Explore P: 0.1356\n",
            "Episode: 634 Total reward: 61.0 Training loss: 6.8223 Explore P: 0.1352\n",
            "Episode: 635 Total reward: 95.0 Training loss: 6.2464 Explore P: 0.1351\n",
            "Model Saved\n",
            "Episode: 636 Total reward: 95.0 Training loss: 33.0388 Explore P: 0.1350\n",
            "Episode: 637 Total reward: 79.0 Training loss: 15.3714 Explore P: 0.1347\n",
            "Episode: 638 Total reward: 72.0 Training loss: 11.7086 Explore P: 0.1344\n",
            "Episode: 639 Total reward: 57.0 Training loss: 29.8869 Explore P: 0.1339\n",
            "Episode: 640 Total reward: 60.0 Training loss: 4.8768 Explore P: 0.1335\n",
            "Model Saved\n",
            "Episode: 641 Total reward: 71.0 Training loss: 15.2262 Explore P: 0.1331\n",
            "Episode: 642 Total reward: 55.0 Training loss: 11.8597 Explore P: 0.1326\n",
            "Episode: 643 Total reward: 78.0 Training loss: 19.7280 Explore P: 0.1323\n",
            "Episode: 644 Total reward: 95.0 Training loss: 6.8418 Explore P: 0.1322\n",
            "Episode: 645 Total reward: 62.0 Training loss: 25.2311 Explore P: 0.1318\n",
            "Model Saved\n",
            "Episode: 646 Total reward: 66.0 Training loss: 7.6437 Explore P: 0.1315\n",
            "Episode: 647 Total reward: 56.0 Training loss: 8.7762 Explore P: 0.1310\n",
            "Episode: 649 Total reward: 74.0 Training loss: 14.0606 Explore P: 0.1301\n",
            "Episode: 650 Total reward: 50.0 Training loss: 9.3536 Explore P: 0.1295\n",
            "Model Saved\n",
            "Episode: 651 Total reward: 74.0 Training loss: 9.2487 Explore P: 0.1293\n",
            "Episode: 652 Total reward: 70.0 Training loss: 15.6611 Explore P: 0.1290\n",
            "Episode: 653 Total reward: 95.0 Training loss: 10.3009 Explore P: 0.1289\n",
            "Episode: 654 Total reward: 65.0 Training loss: 10.6606 Explore P: 0.1285\n",
            "Episode: 655 Total reward: 82.0 Training loss: 24.4638 Explore P: 0.1283\n",
            "Model Saved\n",
            "Episode: 656 Total reward: 66.0 Training loss: 8.4727 Explore P: 0.1280\n",
            "Episode: 657 Total reward: 55.0 Training loss: 7.3013 Explore P: 0.1275\n",
            "Episode: 658 Total reward: 55.0 Training loss: 23.8828 Explore P: 0.1270\n",
            "Episode: 659 Total reward: 68.0 Training loss: 15.4049 Explore P: 0.1267\n",
            "Episode: 660 Total reward: 72.0 Training loss: 13.7648 Explore P: 0.1264\n",
            "Model Saved\n",
            "Episode: 661 Total reward: 67.0 Training loss: 6.0429 Explore P: 0.1260\n",
            "Episode: 662 Total reward: 95.0 Training loss: 10.8070 Explore P: 0.1260\n",
            "Episode: 663 Total reward: 60.0 Training loss: 16.4321 Explore P: 0.1256\n",
            "Episode: 664 Total reward: 65.0 Training loss: 12.0125 Explore P: 0.1252\n",
            "Episode: 665 Total reward: 59.0 Training loss: 12.0701 Explore P: 0.1248\n",
            "Model Saved\n",
            "Episode: 666 Total reward: 66.0 Training loss: 8.7598 Explore P: 0.1244\n",
            "Episode: 667 Total reward: 45.0 Training loss: 8.8325 Explore P: 0.1239\n",
            "Episode: 668 Total reward: 95.0 Training loss: 20.5262 Explore P: 0.1238\n",
            "Episode: 669 Total reward: 85.0 Training loss: 10.3439 Explore P: 0.1237\n",
            "Episode: 670 Total reward: 95.0 Training loss: 13.3213 Explore P: 0.1236\n",
            "Model Saved\n",
            "Episode: 671 Total reward: 87.0 Training loss: 8.5299 Explore P: 0.1234\n",
            "Episode: 672 Total reward: 44.0 Training loss: 14.6293 Explore P: 0.1229\n",
            "Episode: 673 Total reward: 67.0 Training loss: 10.6592 Explore P: 0.1226\n",
            "Episode: 675 Total reward: 63.0 Training loss: 14.3041 Explore P: 0.1216\n",
            "Model Saved\n",
            "Episode: 676 Total reward: 43.0 Training loss: 11.6520 Explore P: 0.1211\n",
            "Episode: 677 Total reward: 95.0 Training loss: 16.3958 Explore P: 0.1210\n",
            "Episode: 678 Total reward: 85.0 Training loss: 9.3429 Explore P: 0.1209\n",
            "Episode: 679 Total reward: 72.0 Training loss: 36.8212 Explore P: 0.1205\n",
            "Episode: 680 Total reward: 95.0 Training loss: 17.4060 Explore P: 0.1205\n",
            "Model Saved\n",
            "Episode: 681 Total reward: 65.0 Training loss: 10.1287 Explore P: 0.1201\n",
            "Episode: 682 Total reward: 50.0 Training loss: 16.0781 Explore P: 0.1197\n",
            "Episode: 683 Total reward: 49.0 Training loss: 11.4239 Explore P: 0.1192\n",
            "Episode: 684 Total reward: 44.0 Training loss: 19.2159 Explore P: 0.1187\n",
            "Episode: 685 Total reward: 61.0 Training loss: 62.9790 Explore P: 0.1183\n",
            "Model Saved\n",
            "Episode: 686 Total reward: 95.0 Training loss: 22.1077 Explore P: 0.1183\n",
            "Episode: 687 Total reward: 86.0 Training loss: 11.2614 Explore P: 0.1181\n",
            "Episode: 688 Total reward: 80.0 Training loss: 13.0965 Explore P: 0.1179\n",
            "Episode: 689 Total reward: 58.0 Training loss: 9.6472 Explore P: 0.1175\n",
            "Episode: 690 Total reward: 74.0 Training loss: 16.1602 Explore P: 0.1172\n",
            "Model Saved\n",
            "Episode: 691 Total reward: 59.0 Training loss: 8.7117 Explore P: 0.1168\n",
            "Episode: 692 Total reward: 78.0 Training loss: 16.5393 Explore P: 0.1165\n",
            "Episode: 693 Total reward: 83.0 Training loss: 12.7426 Explore P: 0.1163\n",
            "Episode: 694 Total reward: 70.0 Training loss: 26.6642 Explore P: 0.1161\n",
            "Episode: 695 Total reward: 86.0 Training loss: 17.2429 Explore P: 0.1159\n",
            "Model Saved\n",
            "Episode: 696 Total reward: 83.0 Training loss: 15.4849 Explore P: 0.1157\n",
            "Episode: 697 Total reward: 66.0 Training loss: 9.6232 Explore P: 0.1154\n",
            "Episode: 698 Total reward: 80.0 Training loss: 23.5504 Explore P: 0.1152\n",
            "Episode: 699 Total reward: 70.0 Training loss: 21.5155 Explore P: 0.1149\n",
            "Episode: 700 Total reward: 70.0 Training loss: 11.3810 Explore P: 0.1145\n",
            "Model Saved\n",
            "Episode: 701 Total reward: 75.0 Training loss: 9.8716 Explore P: 0.1143\n",
            "Episode: 702 Total reward: 45.0 Training loss: 21.7617 Explore P: 0.1138\n",
            "Episode: 703 Total reward: 82.0 Training loss: 34.0012 Explore P: 0.1136\n",
            "Episode: 704 Total reward: 68.0 Training loss: 13.6066 Explore P: 0.1133\n",
            "Episode: 705 Total reward: 73.0 Training loss: 13.5524 Explore P: 0.1131\n",
            "Model Saved\n",
            "Episode: 706 Total reward: 94.0 Training loss: 7.1103 Explore P: 0.1130\n",
            "Episode: 708 Total reward: 70.0 Training loss: 25.1391 Explore P: 0.1123\n",
            "Episode: 709 Total reward: 68.0 Training loss: 11.4480 Explore P: 0.1119\n",
            "Episode: 710 Total reward: 63.0 Training loss: 19.6202 Explore P: 0.1116\n",
            "Model Saved\n",
            "Episode: 711 Total reward: 71.0 Training loss: 15.2081 Explore P: 0.1113\n",
            "Episode: 712 Total reward: 58.0 Training loss: 11.6591 Explore P: 0.1109\n",
            "Episode: 713 Total reward: 93.0 Training loss: 13.9060 Explore P: 0.1109\n",
            "Episode: 714 Total reward: 69.0 Training loss: 8.2327 Explore P: 0.1106\n",
            "Episode: 715 Total reward: 80.0 Training loss: 11.2556 Explore P: 0.1104\n",
            "Model Saved\n",
            "Episode: 716 Total reward: 77.0 Training loss: 18.1991 Explore P: 0.1101\n",
            "Episode: 717 Total reward: 75.0 Training loss: 8.4023 Explore P: 0.1099\n",
            "Episode: 718 Total reward: 86.0 Training loss: 6.5165 Explore P: 0.1097\n",
            "Episode: 719 Total reward: 82.0 Training loss: 15.6727 Explore P: 0.1095\n",
            "Episode: 720 Total reward: 94.0 Training loss: 9.9193 Explore P: 0.1095\n",
            "Model Saved\n",
            "Episode: 721 Total reward: 64.0 Training loss: 13.5598 Explore P: 0.1092\n",
            "Episode: 722 Total reward: 93.0 Training loss: 16.2382 Explore P: 0.1091\n",
            "Episode: 723 Total reward: 94.0 Training loss: 8.6496 Explore P: 0.1090\n",
            "Episode: 724 Total reward: 63.0 Training loss: 23.0116 Explore P: 0.1087\n",
            "Episode: 725 Total reward: 68.0 Training loss: 10.9801 Explore P: 0.1084\n",
            "Model Saved\n",
            "Episode: 726 Total reward: 58.0 Training loss: 5.5436 Explore P: 0.1080\n",
            "Episode: 727 Total reward: 95.0 Training loss: 19.7726 Explore P: 0.1080\n",
            "Episode: 728 Total reward: 91.0 Training loss: 13.8836 Explore P: 0.1079\n",
            "Episode: 729 Total reward: 94.0 Training loss: 12.0413 Explore P: 0.1078\n",
            "Episode: 730 Total reward: 93.0 Training loss: 18.9061 Explore P: 0.1077\n",
            "Model Saved\n",
            "Episode: 731 Total reward: 66.0 Training loss: 17.3662 Explore P: 0.1074\n",
            "Episode: 732 Total reward: 90.0 Training loss: 10.8760 Explore P: 0.1073\n",
            "Episode: 733 Total reward: 67.0 Training loss: 27.3592 Explore P: 0.1070\n",
            "Episode: 734 Total reward: 82.0 Training loss: 11.7105 Explore P: 0.1069\n",
            "Episode: 735 Total reward: 65.0 Training loss: 15.2869 Explore P: 0.1066\n",
            "Model Saved\n",
            "Episode: 736 Total reward: 83.0 Training loss: 7.6721 Explore P: 0.1064\n",
            "Episode: 737 Total reward: 89.0 Training loss: 13.1044 Explore P: 0.1063\n",
            "Episode: 738 Total reward: 95.0 Training loss: 24.9492 Explore P: 0.1062\n",
            "Episode: 739 Total reward: 94.0 Training loss: 15.0179 Explore P: 0.1061\n",
            "Episode: 740 Total reward: 61.0 Training loss: 17.9626 Explore P: 0.1058\n",
            "Model Saved\n",
            "Episode: 741 Total reward: 77.0 Training loss: 27.7397 Explore P: 0.1056\n",
            "Episode: 742 Total reward: 87.0 Training loss: 51.6583 Explore P: 0.1055\n",
            "Episode: 743 Total reward: 95.0 Training loss: 9.6050 Explore P: 0.1054\n",
            "Episode: 744 Total reward: 61.0 Training loss: 6.8019 Explore P: 0.1051\n",
            "Episode: 745 Total reward: 60.0 Training loss: 15.6594 Explore P: 0.1047\n",
            "Model Saved\n",
            "Episode: 746 Total reward: 93.0 Training loss: 31.6499 Explore P: 0.1046\n",
            "Episode: 747 Total reward: 68.0 Training loss: 7.3644 Explore P: 0.1044\n",
            "Episode: 748 Total reward: 57.0 Training loss: 17.8557 Explore P: 0.1040\n",
            "Episode: 749 Total reward: 75.0 Training loss: 8.4744 Explore P: 0.1038\n",
            "Model Saved\n",
            "Episode: 751 Total reward: 70.0 Training loss: 12.9994 Explore P: 0.1031\n",
            "Episode: 752 Total reward: 67.0 Training loss: 25.2068 Explore P: 0.1028\n",
            "Episode: 753 Total reward: 63.0 Training loss: 8.3847 Explore P: 0.1025\n",
            "Episode: 754 Total reward: 77.0 Training loss: 7.7584 Explore P: 0.1023\n",
            "Episode: 755 Total reward: 91.0 Training loss: 7.5200 Explore P: 0.1022\n",
            "Model Saved\n",
            "Episode: 756 Total reward: 53.0 Training loss: 5.7826 Explore P: 0.1017\n",
            "Episode: 757 Total reward: 58.0 Training loss: 8.8613 Explore P: 0.1014\n",
            "Episode: 758 Total reward: 62.0 Training loss: 9.5274 Explore P: 0.1011\n",
            "Episode: 759 Total reward: 61.0 Training loss: 13.8510 Explore P: 0.1007\n",
            "Episode: 760 Total reward: 85.0 Training loss: 12.4389 Explore P: 0.1006\n",
            "Model Saved\n",
            "Episode: 761 Total reward: 69.0 Training loss: 8.0866 Explore P: 0.1004\n",
            "Episode: 762 Total reward: 54.0 Training loss: 9.0040 Explore P: 0.1000\n",
            "Episode: 763 Total reward: 68.0 Training loss: 20.9282 Explore P: 0.0998\n",
            "Episode: 764 Total reward: 91.0 Training loss: 13.4599 Explore P: 0.0997\n",
            "Episode: 765 Total reward: 66.0 Training loss: 16.8020 Explore P: 0.0994\n",
            "Model Saved\n",
            "Episode: 766 Total reward: 91.0 Training loss: 3.7776 Explore P: 0.0993\n",
            "Episode: 767 Total reward: 56.0 Training loss: 11.1536 Explore P: 0.0990\n",
            "Episode: 768 Total reward: 95.0 Training loss: 20.5079 Explore P: 0.0989\n",
            "Episode: 769 Total reward: 90.0 Training loss: 19.0132 Explore P: 0.0988\n",
            "Episode: 770 Total reward: 70.0 Training loss: 9.4254 Explore P: 0.0985\n",
            "Model Saved\n",
            "Episode: 771 Total reward: 64.0 Training loss: 10.0729 Explore P: 0.0983\n",
            "Episode: 772 Total reward: 94.0 Training loss: 20.6774 Explore P: 0.0982\n",
            "Episode: 773 Total reward: 90.0 Training loss: 11.3467 Explore P: 0.0981\n",
            "Episode: 774 Total reward: 48.0 Training loss: 14.2784 Explore P: 0.0977\n",
            "Episode: 775 Total reward: 49.0 Training loss: 9.6365 Explore P: 0.0973\n",
            "Model Saved\n",
            "Episode: 776 Total reward: 41.0 Training loss: 6.1413 Explore P: 0.0969\n",
            "Episode: 777 Total reward: 77.0 Training loss: 6.1807 Explore P: 0.0967\n",
            "Episode: 778 Total reward: 91.0 Training loss: 11.2773 Explore P: 0.0966\n",
            "Episode: 779 Total reward: 85.0 Training loss: 12.4313 Explore P: 0.0964\n",
            "Episode: 780 Total reward: 67.0 Training loss: 12.3560 Explore P: 0.0962\n",
            "Model Saved\n",
            "Episode: 781 Total reward: 85.0 Training loss: 30.7821 Explore P: 0.0961\n",
            "Episode: 782 Total reward: 90.0 Training loss: 21.3453 Explore P: 0.0960\n",
            "Episode: 783 Total reward: 69.0 Training loss: 27.9993 Explore P: 0.0957\n",
            "Episode: 784 Total reward: 78.0 Training loss: 7.3606 Explore P: 0.0955\n",
            "Episode: 785 Total reward: 61.0 Training loss: 4.8326 Explore P: 0.0952\n",
            "Model Saved\n",
            "Episode: 786 Total reward: 61.0 Training loss: 14.3167 Explore P: 0.0949\n",
            "Episode: 787 Total reward: 56.0 Training loss: 11.6884 Explore P: 0.0946\n",
            "Episode: 788 Total reward: 52.0 Training loss: 5.8928 Explore P: 0.0942\n",
            "Episode: 789 Total reward: 95.0 Training loss: 4.9736 Explore P: 0.0942\n",
            "Episode: 790 Total reward: 66.0 Training loss: 9.7184 Explore P: 0.0939\n",
            "Model Saved\n",
            "Episode: 791 Total reward: 95.0 Training loss: 9.3601 Explore P: 0.0939\n",
            "Episode: 792 Total reward: 87.0 Training loss: 14.7525 Explore P: 0.0938\n",
            "Episode: 793 Total reward: 68.0 Training loss: 17.3879 Explore P: 0.0935\n",
            "Episode: 794 Total reward: 43.0 Training loss: 24.8944 Explore P: 0.0931\n",
            "Episode: 795 Total reward: 84.0 Training loss: 9.4502 Explore P: 0.0930\n",
            "Model Saved\n",
            "Episode: 796 Total reward: 94.0 Training loss: 15.4598 Explore P: 0.0929\n",
            "Episode: 797 Total reward: 82.0 Training loss: 32.0980 Explore P: 0.0928\n",
            "Episode: 798 Total reward: 45.0 Training loss: 7.9218 Explore P: 0.0924\n",
            "Episode: 799 Total reward: 95.0 Training loss: 25.4805 Explore P: 0.0923\n",
            "Episode: 800 Total reward: 88.0 Training loss: 16.0383 Explore P: 0.0922\n",
            "Model Saved\n",
            "Episode: 801 Total reward: 55.0 Training loss: 12.0133 Explore P: 0.0919\n",
            "Episode: 802 Total reward: 93.0 Training loss: 33.2336 Explore P: 0.0918\n",
            "Episode: 803 Total reward: 62.0 Training loss: 6.8963 Explore P: 0.0915\n",
            "Episode: 804 Total reward: 95.0 Training loss: 8.8438 Explore P: 0.0915\n",
            "Episode: 805 Total reward: 62.0 Training loss: 16.7023 Explore P: 0.0912\n",
            "Model Saved\n",
            "Episode: 806 Total reward: 47.0 Training loss: 10.9624 Explore P: 0.0909\n",
            "Episode: 807 Total reward: 36.0 Training loss: 26.2043 Explore P: 0.0905\n",
            "Episode: 808 Total reward: 84.0 Training loss: 16.4232 Explore P: 0.0903\n",
            "Episode: 809 Total reward: 60.0 Training loss: 11.9056 Explore P: 0.0900\n",
            "Episode: 810 Total reward: 76.0 Training loss: 6.7605 Explore P: 0.0898\n",
            "Model Saved\n",
            "Episode: 811 Total reward: 84.0 Training loss: 10.0708 Explore P: 0.0897\n",
            "Episode: 812 Total reward: 93.0 Training loss: 5.5392 Explore P: 0.0896\n",
            "Episode: 813 Total reward: 62.0 Training loss: 6.6040 Explore P: 0.0894\n",
            "Episode: 814 Total reward: 75.0 Training loss: 15.8326 Explore P: 0.0892\n",
            "Episode: 815 Total reward: 95.0 Training loss: 31.8016 Explore P: 0.0891\n",
            "Model Saved\n",
            "Episode: 816 Total reward: 86.0 Training loss: 5.4071 Explore P: 0.0890\n",
            "Episode: 817 Total reward: 63.0 Training loss: 24.2685 Explore P: 0.0887\n",
            "Episode: 818 Total reward: 71.0 Training loss: 8.5896 Explore P: 0.0885\n",
            "Episode: 819 Total reward: 87.0 Training loss: 4.0508 Explore P: 0.0884\n",
            "Episode: 820 Total reward: 87.0 Training loss: 4.9388 Explore P: 0.0883\n",
            "Model Saved\n",
            "Episode: 821 Total reward: 92.0 Training loss: 7.1139 Explore P: 0.0882\n",
            "Episode: 822 Total reward: 89.0 Training loss: 35.1941 Explore P: 0.0882\n",
            "Episode: 823 Total reward: 82.0 Training loss: 20.1204 Explore P: 0.0880\n",
            "Episode: 824 Total reward: 93.0 Training loss: 38.8212 Explore P: 0.0879\n",
            "Episode: 825 Total reward: 86.0 Training loss: 9.5050 Explore P: 0.0878\n",
            "Model Saved\n",
            "Episode: 826 Total reward: 57.0 Training loss: 8.9968 Explore P: 0.0875\n",
            "Episode: 827 Total reward: 87.0 Training loss: 8.4430 Explore P: 0.0874\n",
            "Episode: 828 Total reward: 57.0 Training loss: 9.5400 Explore P: 0.0871\n",
            "Episode: 829 Total reward: 71.0 Training loss: 9.7110 Explore P: 0.0869\n",
            "Episode: 830 Total reward: 95.0 Training loss: 13.5740 Explore P: 0.0869\n",
            "Model Saved\n",
            "Episode: 831 Total reward: 77.0 Training loss: 6.8733 Explore P: 0.0867\n",
            "Episode: 832 Total reward: 84.0 Training loss: 10.3885 Explore P: 0.0866\n",
            "Episode: 833 Total reward: 66.0 Training loss: 7.1992 Explore P: 0.0863\n",
            "Episode: 834 Total reward: 86.0 Training loss: 9.8650 Explore P: 0.0862\n",
            "Episode: 835 Total reward: 95.0 Training loss: 43.8017 Explore P: 0.0862\n",
            "Model Saved\n",
            "Episode: 836 Total reward: 93.0 Training loss: 9.5788 Explore P: 0.0861\n",
            "Episode: 837 Total reward: 85.0 Training loss: 11.7222 Explore P: 0.0860\n",
            "Episode: 838 Total reward: 70.0 Training loss: 21.6528 Explore P: 0.0858\n",
            "Episode: 839 Total reward: 62.0 Training loss: 10.2500 Explore P: 0.0855\n",
            "Episode: 840 Total reward: 74.0 Training loss: 12.1634 Explore P: 0.0853\n",
            "Model Saved\n",
            "Episode: 841 Total reward: 72.0 Training loss: 10.1383 Explore P: 0.0852\n",
            "Episode: 842 Total reward: 64.0 Training loss: 9.7457 Explore P: 0.0849\n",
            "Episode: 843 Total reward: 92.0 Training loss: 9.9741 Explore P: 0.0848\n",
            "Episode: 844 Total reward: 64.0 Training loss: 6.6233 Explore P: 0.0846\n",
            "Episode: 845 Total reward: 68.0 Training loss: 33.2516 Explore P: 0.0844\n",
            "Model Saved\n",
            "Episode: 846 Total reward: 73.0 Training loss: 19.6942 Explore P: 0.0842\n",
            "Episode: 847 Total reward: 85.0 Training loss: 8.9274 Explore P: 0.0841\n",
            "Episode: 848 Total reward: 90.0 Training loss: 12.7412 Explore P: 0.0840\n",
            "Episode: 849 Total reward: 65.0 Training loss: 74.8893 Explore P: 0.0837\n",
            "Episode: 850 Total reward: 54.0 Training loss: 14.9429 Explore P: 0.0834\n",
            "Model Saved\n",
            "Episode: 851 Total reward: 77.0 Training loss: 9.1985 Explore P: 0.0832\n",
            "Episode: 852 Total reward: 50.0 Training loss: 94.0696 Explore P: 0.0829\n",
            "Episode: 853 Total reward: 58.0 Training loss: 15.4041 Explore P: 0.0826\n",
            "Episode: 854 Total reward: 95.0 Training loss: 4.9214 Explore P: 0.0826\n",
            "Model Saved\n",
            "Episode: 856 Total reward: 82.0 Training loss: 12.4170 Explore P: 0.0821\n",
            "Episode: 857 Total reward: 59.0 Training loss: 12.7420 Explore P: 0.0818\n",
            "Episode: 858 Total reward: 85.0 Training loss: 44.0839 Explore P: 0.0817\n",
            "Episode: 859 Total reward: 66.0 Training loss: 52.6101 Explore P: 0.0815\n",
            "Episode: 860 Total reward: 93.0 Training loss: 12.5194 Explore P: 0.0814\n",
            "Model Saved\n",
            "Episode: 861 Total reward: 83.0 Training loss: 24.0192 Explore P: 0.0813\n",
            "Episode: 862 Total reward: 65.0 Training loss: 7.0065 Explore P: 0.0810\n",
            "Episode: 863 Total reward: 69.0 Training loss: 7.2044 Explore P: 0.0809\n",
            "Episode: 864 Total reward: 95.0 Training loss: 10.3157 Explore P: 0.0808\n",
            "Episode: 865 Total reward: 95.0 Training loss: 33.0174 Explore P: 0.0808\n",
            "Model Saved\n",
            "Episode: 866 Total reward: 48.0 Training loss: 16.8521 Explore P: 0.0804\n",
            "Episode: 867 Total reward: 60.0 Training loss: 6.6185 Explore P: 0.0802\n",
            "Episode: 868 Total reward: 53.0 Training loss: 14.5584 Explore P: 0.0799\n",
            "Episode: 869 Total reward: 82.0 Training loss: 17.8925 Explore P: 0.0797\n",
            "Episode: 870 Total reward: 71.0 Training loss: 18.7190 Explore P: 0.0796\n",
            "Model Saved\n",
            "Episode: 871 Total reward: 70.0 Training loss: 11.7515 Explore P: 0.0794\n",
            "Episode: 872 Total reward: 75.0 Training loss: 8.4301 Explore P: 0.0792\n",
            "Episode: 873 Total reward: 86.0 Training loss: 12.0587 Explore P: 0.0791\n",
            "Episode: 874 Total reward: 55.0 Training loss: 6.5275 Explore P: 0.0788\n",
            "Episode: 875 Total reward: 80.0 Training loss: 8.0875 Explore P: 0.0787\n",
            "Model Saved\n",
            "Episode: 876 Total reward: 67.0 Training loss: 6.6929 Explore P: 0.0785\n",
            "Episode: 877 Total reward: 64.0 Training loss: 10.0913 Explore P: 0.0783\n",
            "Episode: 878 Total reward: 65.0 Training loss: 11.4417 Explore P: 0.0780\n",
            "Episode: 879 Total reward: 77.0 Training loss: 10.8844 Explore P: 0.0779\n",
            "Episode: 880 Total reward: 82.0 Training loss: 10.6317 Explore P: 0.0778\n",
            "Model Saved\n",
            "Episode: 881 Total reward: 55.0 Training loss: 30.8904 Explore P: 0.0775\n",
            "Episode: 882 Total reward: 48.0 Training loss: 14.4019 Explore P: 0.0772\n",
            "Episode: 883 Total reward: 83.0 Training loss: 7.8220 Explore P: 0.0771\n",
            "Episode: 884 Total reward: 75.0 Training loss: 6.3356 Explore P: 0.0769\n",
            "Episode: 885 Total reward: 60.0 Training loss: 27.6317 Explore P: 0.0767\n",
            "Model Saved\n",
            "Episode: 886 Total reward: 68.0 Training loss: 13.8278 Explore P: 0.0765\n",
            "Episode: 887 Total reward: 45.0 Training loss: 6.1277 Explore P: 0.0762\n",
            "Episode: 888 Total reward: 81.0 Training loss: 13.8008 Explore P: 0.0760\n",
            "Episode: 890 Total reward: 59.0 Training loss: 14.1690 Explore P: 0.0755\n",
            "Model Saved\n",
            "Episode: 891 Total reward: 82.0 Training loss: 9.5120 Explore P: 0.0753\n",
            "Episode: 892 Total reward: 79.0 Training loss: 7.8576 Explore P: 0.0752\n",
            "Episode: 893 Total reward: 63.0 Training loss: 18.6756 Explore P: 0.0750\n",
            "Episode: 894 Total reward: 77.0 Training loss: 16.3735 Explore P: 0.0748\n",
            "Episode: 895 Total reward: 55.0 Training loss: 10.1476 Explore P: 0.0746\n",
            "Model Saved\n",
            "Episode: 896 Total reward: 85.0 Training loss: 4.1518 Explore P: 0.0745\n",
            "Episode: 897 Total reward: 86.0 Training loss: 6.6606 Explore P: 0.0744\n",
            "Episode: 898 Total reward: 87.0 Training loss: 5.9774 Explore P: 0.0743\n",
            "Episode: 899 Total reward: 85.0 Training loss: 17.1742 Explore P: 0.0742\n",
            "Episode: 900 Total reward: 91.0 Training loss: 5.4413 Explore P: 0.0741\n",
            "Model Saved\n",
            "Episode: 901 Total reward: 83.0 Training loss: 11.3495 Explore P: 0.0740\n",
            "Episode: 902 Total reward: 90.0 Training loss: 7.5589 Explore P: 0.0739\n",
            "Episode: 903 Total reward: 95.0 Training loss: 2.3686 Explore P: 0.0739\n",
            "Episode: 904 Total reward: 56.0 Training loss: 4.0763 Explore P: 0.0736\n",
            "Episode: 905 Total reward: 82.0 Training loss: 16.7176 Explore P: 0.0735\n",
            "Model Saved\n",
            "Episode: 906 Total reward: 84.0 Training loss: 4.3659 Explore P: 0.0734\n",
            "Episode: 907 Total reward: 57.0 Training loss: 13.7335 Explore P: 0.0731\n",
            "Episode: 908 Total reward: 80.0 Training loss: 17.5929 Explore P: 0.0730\n",
            "Episode: 909 Total reward: 63.0 Training loss: 17.6126 Explore P: 0.0728\n",
            "Episode: 910 Total reward: 95.0 Training loss: 8.6967 Explore P: 0.0728\n",
            "Model Saved\n",
            "Episode: 911 Total reward: 86.0 Training loss: 3.2082 Explore P: 0.0727\n",
            "Episode: 912 Total reward: 85.0 Training loss: 6.0494 Explore P: 0.0726\n",
            "Episode: 913 Total reward: 87.0 Training loss: 13.9004 Explore P: 0.0725\n",
            "Episode: 914 Total reward: 68.0 Training loss: 17.3912 Explore P: 0.0723\n",
            "Episode: 915 Total reward: 93.0 Training loss: 5.1194 Explore P: 0.0723\n",
            "Model Saved\n",
            "Episode: 916 Total reward: 68.0 Training loss: 14.5395 Explore P: 0.0721\n",
            "Episode: 917 Total reward: 75.0 Training loss: 8.5729 Explore P: 0.0719\n",
            "Episode: 919 Total reward: 94.0 Training loss: 8.7461 Explore P: 0.0716\n",
            "Episode: 920 Total reward: 57.0 Training loss: 9.7363 Explore P: 0.0713\n",
            "Model Saved\n",
            "Episode: 921 Total reward: 48.0 Training loss: 7.4982 Explore P: 0.0710\n",
            "Episode: 922 Total reward: 75.0 Training loss: 7.0191 Explore P: 0.0709\n",
            "Episode: 923 Total reward: 80.0 Training loss: 11.2491 Explore P: 0.0708\n",
            "Episode: 924 Total reward: 95.0 Training loss: 5.8828 Explore P: 0.0707\n",
            "Episode: 925 Total reward: 86.0 Training loss: 6.5276 Explore P: 0.0706\n",
            "Model Saved\n",
            "Episode: 926 Total reward: 95.0 Training loss: 11.6238 Explore P: 0.0706\n",
            "Episode: 927 Total reward: 68.0 Training loss: 5.9479 Explore P: 0.0704\n",
            "Episode: 928 Total reward: 78.0 Training loss: 17.6184 Explore P: 0.0703\n",
            "Episode: 929 Total reward: 79.0 Training loss: 25.9994 Explore P: 0.0701\n",
            "Episode: 930 Total reward: 86.0 Training loss: 7.8047 Explore P: 0.0700\n",
            "Model Saved\n",
            "Episode: 931 Total reward: 81.0 Training loss: 8.9928 Explore P: 0.0699\n",
            "Episode: 932 Total reward: 95.0 Training loss: 8.8081 Explore P: 0.0699\n",
            "Episode: 933 Total reward: 43.0 Training loss: 6.1403 Explore P: 0.0696\n",
            "Episode: 934 Total reward: 81.0 Training loss: 8.6777 Explore P: 0.0695\n",
            "Episode: 935 Total reward: 87.0 Training loss: 17.9667 Explore P: 0.0694\n",
            "Model Saved\n",
            "Episode: 936 Total reward: 68.0 Training loss: 34.1281 Explore P: 0.0692\n",
            "Episode: 937 Total reward: 86.0 Training loss: 4.3912 Explore P: 0.0691\n",
            "Episode: 938 Total reward: 52.0 Training loss: 16.5528 Explore P: 0.0689\n",
            "Episode: 939 Total reward: 93.0 Training loss: 7.7679 Explore P: 0.0688\n",
            "Episode: 940 Total reward: 67.0 Training loss: 12.6069 Explore P: 0.0687\n",
            "Model Saved\n",
            "Episode: 941 Total reward: 77.0 Training loss: 2.8251 Explore P: 0.0685\n",
            "Episode: 942 Total reward: 95.0 Training loss: 9.2872 Explore P: 0.0685\n",
            "Episode: 943 Total reward: 63.0 Training loss: 3.7070 Explore P: 0.0683\n",
            "Episode: 944 Total reward: 58.0 Training loss: 3.4463 Explore P: 0.0681\n",
            "Episode: 945 Total reward: 66.0 Training loss: 13.0366 Explore P: 0.0679\n",
            "Model Saved\n",
            "Episode: 946 Total reward: 57.0 Training loss: 4.5348 Explore P: 0.0677\n",
            "Episode: 947 Total reward: 81.0 Training loss: 9.0638 Explore P: 0.0675\n",
            "Episode: 948 Total reward: 81.0 Training loss: 7.5663 Explore P: 0.0674\n",
            "Episode: 949 Total reward: 61.0 Training loss: 10.9939 Explore P: 0.0672\n",
            "Model Saved\n",
            "Episode: 951 Total reward: 95.0 Training loss: 4.5149 Explore P: 0.0669\n",
            "Episode: 952 Total reward: 95.0 Training loss: 4.7213 Explore P: 0.0669\n",
            "Episode: 953 Total reward: 53.0 Training loss: 7.6634 Explore P: 0.0666\n",
            "Episode: 954 Total reward: 83.0 Training loss: 5.0122 Explore P: 0.0665\n",
            "Episode: 955 Total reward: 72.0 Training loss: 19.4933 Explore P: 0.0664\n",
            "Model Saved\n",
            "Episode: 956 Total reward: 52.0 Training loss: 21.5405 Explore P: 0.0661\n",
            "Episode: 957 Total reward: 61.0 Training loss: 14.9474 Explore P: 0.0659\n",
            "Episode: 958 Total reward: 88.0 Training loss: 12.1542 Explore P: 0.0659\n",
            "Episode: 959 Total reward: 60.0 Training loss: 8.7839 Explore P: 0.0657\n",
            "Episode: 960 Total reward: 93.0 Training loss: 14.2473 Explore P: 0.0656\n",
            "Model Saved\n",
            "Episode: 961 Total reward: 76.0 Training loss: 3.5971 Explore P: 0.0655\n",
            "Episode: 962 Total reward: 58.0 Training loss: 19.2806 Explore P: 0.0653\n",
            "Episode: 963 Total reward: 42.0 Training loss: 16.6090 Explore P: 0.0650\n",
            "Episode: 964 Total reward: 75.0 Training loss: 18.1439 Explore P: 0.0649\n",
            "Episode: 965 Total reward: 48.0 Training loss: 9.1929 Explore P: 0.0646\n",
            "Model Saved\n",
            "Episode: 966 Total reward: 84.0 Training loss: 12.1639 Explore P: 0.0645\n",
            "Episode: 967 Total reward: 79.0 Training loss: 13.1891 Explore P: 0.0644\n",
            "Episode: 968 Total reward: 73.0 Training loss: 8.7742 Explore P: 0.0642\n",
            "Episode: 969 Total reward: 61.0 Training loss: 16.1216 Explore P: 0.0641\n",
            "Episode: 970 Total reward: 76.0 Training loss: 9.5614 Explore P: 0.0639\n",
            "Model Saved\n",
            "Episode: 971 Total reward: 88.0 Training loss: 14.8693 Explore P: 0.0639\n",
            "Episode: 973 Total reward: 91.0 Training loss: 15.6028 Explore P: 0.0635\n",
            "Episode: 974 Total reward: 91.0 Training loss: 13.3151 Explore P: 0.0635\n",
            "Episode: 975 Total reward: 54.0 Training loss: 9.9744 Explore P: 0.0633\n",
            "Model Saved\n",
            "Episode: 976 Total reward: 63.0 Training loss: 8.9128 Explore P: 0.0631\n",
            "Episode: 977 Total reward: 66.0 Training loss: 6.4060 Explore P: 0.0629\n",
            "Episode: 978 Total reward: 54.0 Training loss: 28.8317 Explore P: 0.0627\n",
            "Episode: 979 Total reward: 80.0 Training loss: 30.9630 Explore P: 0.0626\n",
            "Episode: 980 Total reward: 82.0 Training loss: 17.6478 Explore P: 0.0625\n",
            "Model Saved\n",
            "Episode: 981 Total reward: 66.0 Training loss: 5.7800 Explore P: 0.0623\n",
            "Episode: 982 Total reward: 77.0 Training loss: 4.8782 Explore P: 0.0622\n",
            "Episode: 983 Total reward: 68.0 Training loss: 7.5535 Explore P: 0.0620\n",
            "Episode: 984 Total reward: 74.0 Training loss: 4.7976 Explore P: 0.0619\n",
            "Episode: 985 Total reward: 85.0 Training loss: 39.6476 Explore P: 0.0618\n",
            "Model Saved\n",
            "Episode: 986 Total reward: 64.0 Training loss: 17.3619 Explore P: 0.0616\n",
            "Episode: 987 Total reward: 81.0 Training loss: 11.3442 Explore P: 0.0615\n",
            "Episode: 988 Total reward: 86.0 Training loss: 42.3994 Explore P: 0.0615\n",
            "Episode: 989 Total reward: 87.0 Training loss: 23.4155 Explore P: 0.0614\n",
            "Episode: 990 Total reward: 68.0 Training loss: 13.3600 Explore P: 0.0612\n",
            "Model Saved\n",
            "Episode: 991 Total reward: 74.0 Training loss: 6.4663 Explore P: 0.0611\n",
            "Episode: 992 Total reward: 46.0 Training loss: 11.5128 Explore P: 0.0609\n",
            "Episode: 993 Total reward: 82.0 Training loss: 6.8041 Explore P: 0.0608\n",
            "Episode: 994 Total reward: 53.0 Training loss: 10.7341 Explore P: 0.0606\n",
            "Episode: 995 Total reward: 91.0 Training loss: 29.0299 Explore P: 0.0605\n",
            "Model Saved\n",
            "Episode: 996 Total reward: 91.0 Training loss: 5.2299 Explore P: 0.0605\n",
            "Episode: 997 Total reward: 78.0 Training loss: 6.4340 Explore P: 0.0603\n",
            "Episode: 998 Total reward: 69.0 Training loss: 4.8991 Explore P: 0.0602\n",
            "Episode: 999 Total reward: 87.0 Training loss: 21.0592 Explore P: 0.0601\n"
          ]
        }
      ],
      "source": [
        "# Saver will help us to save our model\n",
        "saver = tf.compat.v1.train.Saver()\n",
        "\n",
        "if training == True:\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        # Initialize the variables\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        # Initialize the decay rate (that will use to reduce epsilon)\n",
        "        decay_step = 0\n",
        "\n",
        "        # Init the game\n",
        "        game.init()\n",
        "\n",
        "        for episode in range(total_episodes):\n",
        "            # Set step to 0\n",
        "            step = 0\n",
        "\n",
        "            # Initialize the rewards of the episode\n",
        "            episode_rewards = []\n",
        "\n",
        "            # Make a new episode and observe the first state\n",
        "            game.new_episode()\n",
        "            state = game.get_state().screen_buffer\n",
        "\n",
        "            # Remember that stack frame function also call our preprocess function.\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "            while step < max_steps:\n",
        "                step += 1\n",
        "\n",
        "                # Increase decay_step\n",
        "                decay_step +=1\n",
        "\n",
        "                # Predict the action to take and take it\n",
        "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
        "\n",
        "                # Do the action\n",
        "                reward = game.make_action(action)\n",
        "\n",
        "                # Look if the episode is finished\n",
        "                done = game.is_episode_finished()\n",
        "\n",
        "                # Add the reward to total reward\n",
        "                episode_rewards.append(reward)\n",
        "\n",
        "                # If the game is finished\n",
        "                if done:\n",
        "                    # the episode ends so no next state\n",
        "                    next_state = np.zeros((84,84), dtype=np.int)\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "                    # Set step = max_steps to end the episode\n",
        "                    step = max_steps\n",
        "\n",
        "                    # Get the total reward of the episode\n",
        "                    total_reward = np.sum(episode_rewards)\n",
        "\n",
        "                    print('Episode: {}'.format(episode),\n",
        "                              'Total reward: {}'.format(total_reward),\n",
        "                              'Training loss: {:.4f}'.format(loss),\n",
        "                              'Explore P: {:.4f}'.format(explore_probability))\n",
        "\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                else:\n",
        "                    # Get the next state\n",
        "                    next_state = game.get_state().screen_buffer\n",
        "\n",
        "                    # Stack the frame of the next_state\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "\n",
        "                    # Add experience to memory\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                    # st+1 is now our current state\n",
        "                    state = next_state\n",
        "\n",
        "\n",
        "                ### LEARNING PART\n",
        "                # Obtain random mini-batch from memory\n",
        "                batch = memory.sample(batch_size)\n",
        "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "                actions_mb = np.array([each[1] for each in batch])\n",
        "                rewards_mb = np.array([each[2] for each in batch])\n",
        "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "                dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                target_Qs_batch = []\n",
        "\n",
        "                 # Get Q values for next_state\n",
        "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
        "\n",
        "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
        "                for i in range(0, len(batch)):\n",
        "                    terminal = dones_mb[i]\n",
        "\n",
        "                    # If we are in a terminal state, only equals reward\n",
        "                    if terminal:\n",
        "                        target_Qs_batch.append(rewards_mb[i])\n",
        "\n",
        "                    else:\n",
        "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "                        target_Qs_batch.append(target)\n",
        "\n",
        "\n",
        "                targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
        "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                               DQNetwork.target_Q: targets_mb,\n",
        "                                               DQNetwork.actions_: actions_mb})\n",
        "\n",
        "                # Write TF Summaries\n",
        "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                   DQNetwork.target_Q: targets_mb,\n",
        "                                                   DQNetwork.actions_: actions_mb})\n",
        "                writer.add_summary(summary, episode)\n",
        "                writer.flush()\n",
        "\n",
        "            # Save model every 5 episodes\n",
        "            if episode % 5 == 0:\n",
        "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
        "                print(\"Model Saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9CaMf1ojy2E"
      },
      "source": [
        "## Step 9: Watch our Agent play üëÄ\n",
        "Now that we trained our agent, we can test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_qVzNR_jy2E",
        "outputId": "9f9fb161-972e-4b49-d070-515612ab0de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "else\n",
            "Score:  62.0\n"
          ]
        }
      ],
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "\n",
        "    game, possible_actions = create_environment()\n",
        "\n",
        "    totalScore = 0\n",
        "\n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "    game.init()\n",
        "    for i in range(1):\n",
        "\n",
        "        done = False\n",
        "\n",
        "        game.new_episode()\n",
        "\n",
        "        state = game.get_state().screen_buffer\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "\n",
        "        while not game.is_episode_finished():\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            choice = np.argmax(Qs)\n",
        "            action = possible_actions[int(choice)]\n",
        "\n",
        "            game.make_action(action)\n",
        "            done = game.is_episode_finished()\n",
        "            score = game.get_total_reward()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"else\")\n",
        "                next_state = game.get_state().screen_buffer\n",
        "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                state = next_state\n",
        "\n",
        "        score = game.get_total_reward()\n",
        "        print(\"Score: \", score)\n",
        "    game.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}